{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94bb3daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/miniconda2/envs/iot/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJ_PATH=/home/hoang/github/TS-IDS\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, re, datetime, random, gzip, json, copy\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from itertools import accumulate\n",
    "import argparse\n",
    "from time import time\n",
    "from math import ceil\n",
    "from collections import Counter\n",
    "import socket,struct\n",
    "import timeit\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score, accuracy_score, top_k_accuracy_score, roc_auc_score\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import category_encoders as ce\n",
    "\n",
    "from dgl import from_networkx\n",
    "import dgl.function as fn\n",
    "import networkx as nx\n",
    "\n",
    "PROJ_PATH = Path(os.path.join(re.sub(\"/TS-IDS.*$\", '', os.getcwd()), 'TS-IDS'))\n",
    "print(f'PROJ_PATH={PROJ_PATH}')\n",
    "sys.path.insert(1, str(PROJ_PATH))\n",
    "sys.path.insert(1, str(PROJ_PATH/'src'))\n",
    "import utils\n",
    "from utils import *\n",
    "from dataset import build_datamodule\n",
    "from trainer import build_trainer\n",
    "from model import TSIDS\n",
    "from pipeline import TSIDSPipeline\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15e63f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(pred, labels):\n",
    "    return (pred.argmax(1) == labels).float().mean().item()\n",
    "\n",
    "class SAGELayer(nn.Module):\n",
    "    def __init__(self, ndim_in, edims, ndim_out, activation):\n",
    "        super(SAGELayer, self).__init__()\n",
    "        ### force to outut fix dimensions\n",
    "        self.W_msg = nn.Linear(ndim_in + edims, ndim_out)\n",
    "        ### apply weight\n",
    "        self.W_apply = nn.Linear(ndim_in + ndim_out, ndim_out)\n",
    "        self.activation = activation\n",
    "\n",
    "    def message_func(self, edges):\n",
    "        return {'m': self.W_msg(torch.cat([edges.src['h'], edges.data['h']], 2))}\n",
    "\n",
    "    def forward(self, g_dgl, nfeats, efeats):\n",
    "        with g_dgl.local_scope():\n",
    "            g = g_dgl\n",
    "            g.ndata['h'] = nfeats\n",
    "            g.edata['h'] = efeats\n",
    "            # Eq4\n",
    "            g.update_all(self.message_func, fn.mean('m', 'h_neigh'))\n",
    "            # Eq5          \n",
    "            g.ndata['h'] = F.relu(self.W_apply(torch.cat([g.ndata['h'], g.ndata['h_neigh']], 2)))\n",
    "            return g.ndata['h']\n",
    "\n",
    "\n",
    "class SAGE(nn.Module):\n",
    "    def __init__(self, ndim_in, ndim_out, edim, activation, dropout):\n",
    "        super(SAGE, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(SAGELayer(ndim_in, edim, 128, activation))\n",
    "        self.layers.append(SAGELayer(128, edim, ndim_out, activation))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, g, nfeats, efeats):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i != 0:\n",
    "                nfeats = self.dropout(nfeats)\n",
    "            nfeats = layer(g, nfeats, efeats)\n",
    "        return nfeats.sum(1)\n",
    "    \n",
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, in_features, out_classes):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(in_features * 2, out_classes)\n",
    "\n",
    "    def apply_edges(self, edges):\n",
    "        h_u = edges.src['h']\n",
    "        h_v = edges.dst['h']\n",
    "        score = self.W(torch.cat([h_u, h_v], 1))\n",
    "        return {'score': score}\n",
    "\n",
    "    def forward(self, graph, h):\n",
    "        with graph.local_scope():\n",
    "            graph.ndata['h'] = h\n",
    "            graph.apply_edges(self.apply_edges)\n",
    "            return graph.edata['score']\n",
    "        \n",
    "class Model(nn.Module):\n",
    "    def __init__(self, ndim_in, ndim_out, edim, n_classes, activation, dropout):\n",
    "        super().__init__()\n",
    "        self.gnn = SAGE(ndim_in, ndim_out, edim, activation, dropout)\n",
    "        self.pred = MLPPredictor(ndim_out, n_classes)\n",
    "    def forward(self, g, nfeats, efeats):\n",
    "        h = self.gnn(g, nfeats, efeats)\n",
    "        return self.pred(g, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de9dd68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(scaler, encoder, X, y, cols_to_norm):\n",
    "    X = encoder.transform(X)\n",
    "    print(cols_to_norm)\n",
    "    X[cols_to_norm] = scaler.transform(X[cols_to_norm])\n",
    "    X['h'] = X[cols_to_norm].values.tolist()\n",
    "    X['h'] = X['h'].apply(lambda x: torch.tensor(x))\n",
    "    \n",
    "    G = nx.from_pandas_edgelist(\n",
    "    X, \"IPV4_SRC_ADDR\", \"IPV4_DST_ADDR\", ['h', cname_label], create_using=nx.DiGraph())\n",
    "#     G = nx.from_pandas_edgelist(\n",
    "#         X, \"IPV4_SRC_ADDR\", \"IPV4_DST_ADDR\", ['h', cname_label], create_using=nx.MultiGraph())\n",
    "#     G = G.to_directed()\n",
    "    G = from_networkx(G, edge_attrs=['h', cname_label])\n",
    "    \n",
    "    # Eq1\n",
    "    G.ndata['h'] = torch.ones(G.num_nodes(), G.edata['h'].shape[1])\n",
    "    G.edata['train_mask'] = torch.ones(len(G.edata['h']), dtype=torch.bool)\n",
    "    \n",
    "    G.ndata['h'] = torch.reshape(G.ndata['h'], (G.ndata['h'].shape[0], 1,G.ndata['h'].shape[1]))\n",
    "    G.edata['h'] = torch.reshape(G.edata['h'], (G.edata['h'].shape[0], 1,G.edata['h'].shape[1]))\n",
    "    G = G.to(device)\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa165c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prob_df(\n",
    "    tvt_str,\n",
    "    model,\n",
    "    G,\n",
    "    node_features, \n",
    "    edge_features,\n",
    "    actual\n",
    "):\n",
    "    pred_prop = model(G, node_features, edge_features)\n",
    "    norm_pred_prop = torch.softmax(pred_prop, dim=1)\n",
    "    data_array = [pred_prop_ + [actual_, tvt_str] for pred_prop_, actual_ in zip(norm_pred_prop.tolist(), actual.tolist())]\n",
    "    cnames = [f'probs_{i}' for i in range(norm_pred_prop.shape[1])]\n",
    "    prob_df = pd.DataFrame(data_array, columns=cnames+['gts', 'tvt'])\n",
    "    return prob_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8807b01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_baseline(\n",
    "    ds_name,\n",
    "    cname_label,\n",
    "    n_epochs\n",
    "):\n",
    "    cname_tvt = f'{cname_label}_tvt'\n",
    "    \n",
    "    data = pd.read_csv(f'../datasets/{ds_name}_tvt.csv')\n",
    "    label2idx = pd.read_pickle(f'../datasets/{ds_name}_graph_multi.pkl')['label2idx']\n",
    "    if cname_label == 'Attack':\n",
    "        data['Attack'] = data['Attack'].map(label2idx)\n",
    "        \n",
    "    ####\n",
    "    data['IPV4_SRC_ADDR'] = data.IPV4_SRC_ADDR.apply(\n",
    "        lambda x: socket.inet_ntoa(struct.pack('>I', random.randint(0xac100001, 0xac1f0001))))\n",
    "    data['IPV4_SRC_ADDR'] = data.IPV4_SRC_ADDR.apply(str)\n",
    "    data['L4_SRC_PORT'] = data.L4_SRC_PORT.apply(str)\n",
    "    data['IPV4_DST_ADDR'] = data.IPV4_DST_ADDR.apply(str)\n",
    "    data['L4_DST_PORT'] = data.L4_DST_PORT.apply(str)\n",
    "\n",
    "    data['IPV4_SRC_ADDR'] = data['IPV4_SRC_ADDR'] + ':' + data['L4_SRC_PORT']\n",
    "    data['IPV4_DST_ADDR'] = data['IPV4_DST_ADDR'] + ':' + data['L4_DST_PORT']\n",
    "\n",
    "    data.drop(columns=['L4_SRC_PORT','L4_DST_PORT'], inplace=True)\n",
    "    \n",
    "    ####\n",
    "    X_cnames = [c for c in data.columns if c not in ['Label_tvt', 'Attack_tvt']]\n",
    "    X_train, X_test, y_train, y_test = (data[data[cname_tvt]!='test'][X_cnames], \n",
    "                                        data[data[cname_tvt]=='test'][X_cnames], \n",
    "                                        data[data[cname_tvt]!='test'][cname_label], \n",
    "                                        data[data[cname_tvt]=='test'][cname_label])\n",
    "    \n",
    "    ####\n",
    "    cols_to_norm = list(set(X_train.columns) - set(['Label', 'Attack', 'IPV4_SRC_ADDR', 'IPV4_DST_ADDR']))\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train[cols_to_norm])\n",
    "    encoder = ce.TargetEncoder(cols=['TCP_FLAGS','L7_PROTO','PROTOCOL'])\n",
    "    encoder.fit(X_train, y_train)\n",
    "\n",
    "    G_train = build_graph(scaler, encoder, X_train, y_train, cols_to_norm)\n",
    "    G_test = build_graph(scaler, encoder, X_test, y_test, cols_to_norm)\n",
    "\n",
    "    node_features = G_train.ndata['h']\n",
    "    edge_features = G_train.edata['h']\n",
    "\n",
    "    node_features_test = G_test.ndata['h']\n",
    "    edge_features_test = G_test.edata['h']\n",
    "    \n",
    "    ####\n",
    "    ndim_in = G_train.ndata['h'].shape[2]\n",
    "    ndim_out = 128 \n",
    "    edim = G_train.ndata['h'].shape[2]\n",
    "    activation = F.relu\n",
    "    dropout = 0.2\n",
    "    n_classes = data[cname_label].nunique()\n",
    "\n",
    "    sage = SAGE(ndim_in, ndim_out, edim, activation, dropout).to(device)\n",
    "    mlp = MLPPredictor(ndim_out, 2).to(device)\n",
    "    model = Model(ndim_in, ndim_out, edim, n_classes, activation, dropout).to(device)\n",
    "    opt = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    class_weights = class_weight.compute_class_weight(\n",
    "        'balanced', \n",
    "        classes=data[cname_label].unique(),\n",
    "        y=data[cname_label].values.tolist(),\n",
    "    )\n",
    "    class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    \n",
    "    ####\n",
    "    edge_label = G_train.edata[cname_label]\n",
    "    train_mask = G_train.edata['train_mask']\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        pred = model(G_train, node_features, edge_features).to(device)\n",
    "        loss = criterion(pred[train_mask], edge_label[train_mask])\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'{epoch:04d} - Training acc:', compute_accuracy(pred[train_mask], edge_label[train_mask]))\n",
    "            \n",
    "    #### test\n",
    "    test_pred_prop = model(G_test, node_features_test, edge_features_test).to(device)\n",
    "    norm_test_pred_prop = torch.softmax(test_pred_prop, dim=1)\n",
    "    test_pred = test_pred_prop.argmax(1)\n",
    "    test_pred = torch.Tensor.cpu(test_pred).detach().numpy()\n",
    "    test_actual = G_test.edata.pop(cname_label)\n",
    "\n",
    "    #### train\n",
    "    train_pred_prop = model(G_train, node_features, edge_features).to(device)\n",
    "    train_actual = G_train.edata.pop(cname_label)\n",
    "    print(type(train_actual))\n",
    "    print(len(train_actual))\n",
    "    print(len(test_actual))\n",
    "    #### create probs df\n",
    "    test_prob_df = create_prob_df(\n",
    "        'train',\n",
    "        model,\n",
    "        G_train,\n",
    "        node_features,\n",
    "        edge_features,\n",
    "        train_actual\n",
    "    )\n",
    "    train_prob_df = create_prob_df(\n",
    "        'test',\n",
    "        model,\n",
    "        G_test,\n",
    "        node_features_test,\n",
    "        edge_features_test,\n",
    "        test_actual\n",
    "    )\n",
    "    prob_df = pd.concat([test_prob_df, train_prob_df], axis=0)\n",
    "    return prob_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bbad3d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# BoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f236165",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ds_name = 'NF-BoT-IoT'\n",
    "n_epochs = 5000\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15cb14b5",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/miniconda2/envs/iot/lib/python3.9/site-packages/category_encoders/target_encoder.py:92: FutureWarning: Default parameter min_samples_leaf will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter min_samples_leaf will change in version 2.6.\"\n",
      "/home/hoang/miniconda2/envs/iot/lib/python3.9/site-packages/category_encoders/target_encoder.py:97: FutureWarning: Default parameter smoothing will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter smoothing will change in version 2.6.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FLOW_DURATION_MILLISECONDS', 'PROTOCOL', 'L7_PROTO', 'OUT_BYTES', 'IN_BYTES', 'TCP_FLAGS', 'IN_PKTS', 'OUT_PKTS']\n",
      "['FLOW_DURATION_MILLISECONDS', 'PROTOCOL', 'L7_PROTO', 'OUT_BYTES', 'IN_BYTES', 'TCP_FLAGS', 'IN_PKTS', 'OUT_PKTS']\n",
      "0010 - Training acc: 0.9569141864776611\n",
      "0020 - Training acc: 0.7235525250434875\n",
      "0030 - Training acc: 0.9064415097236633\n",
      "0040 - Training acc: 0.8148398995399475\n",
      "0050 - Training acc: 0.841542661190033\n",
      "0060 - Training acc: 0.7265877723693848\n",
      "0070 - Training acc: 0.687746524810791\n",
      "0080 - Training acc: 0.794643223285675\n",
      "0090 - Training acc: 0.7439563274383545\n",
      "0100 - Training acc: 0.5243304967880249\n",
      "0110 - Training acc: 0.7317202687263489\n",
      "0120 - Training acc: 0.7791719436645508\n",
      "0130 - Training acc: 0.7395641803741455\n",
      "0140 - Training acc: 0.8919844031333923\n",
      "0150 - Training acc: 0.8717424869537354\n",
      "0160 - Training acc: 0.7458108067512512\n",
      "0170 - Training acc: 0.7576683759689331\n",
      "0180 - Training acc: 0.6392378211021423\n",
      "0190 - Training acc: 0.7012990713119507\n",
      "0200 - Training acc: 0.6162011027336121\n",
      "0210 - Training acc: 0.8894990682601929\n",
      "0220 - Training acc: 0.5784454345703125\n",
      "0230 - Training acc: 0.6383974552154541\n",
      "0240 - Training acc: 0.8258928656578064\n",
      "0250 - Training acc: 0.8446112275123596\n",
      "0260 - Training acc: 0.7550830841064453\n",
      "0270 - Training acc: 0.8418759703636169\n",
      "0280 - Training acc: 0.941297709941864\n",
      "0290 - Training acc: 0.8542001247406006\n",
      "0300 - Training acc: 0.8992522358894348\n",
      "0310 - Training acc: 0.7257474064826965\n",
      "0320 - Training acc: 0.9228531122207642\n",
      "0330 - Training acc: 0.8766964077949524\n",
      "0340 - Training acc: 0.8095669746398926\n",
      "0350 - Training acc: 0.766107439994812\n",
      "0360 - Training acc: 0.949920117855072\n",
      "0370 - Training acc: 0.8753109574317932\n",
      "0380 - Training acc: 0.8985428214073181\n",
      "0390 - Training acc: 0.9199702739715576\n",
      "0400 - Training acc: 0.9236244559288025\n",
      "0410 - Training acc: 0.8886420726776123\n",
      "0420 - Training acc: 0.8422925472259521\n",
      "0430 - Training acc: 0.8321465849876404\n",
      "0440 - Training acc: 0.8016992211341858\n",
      "0450 - Training acc: 0.9066367149353027\n",
      "0460 - Training acc: 0.9251480102539062\n",
      "0470 - Training acc: 0.9183086156845093\n",
      "0480 - Training acc: 0.6679354906082153\n",
      "0490 - Training acc: 0.9234315752983093\n",
      "0500 - Training acc: 0.9420238137245178\n",
      "0510 - Training acc: 0.8820978999137878\n",
      "0520 - Training acc: 0.9287283420562744\n",
      "0530 - Training acc: 0.8464394807815552\n",
      "0540 - Training acc: 0.8669646978378296\n",
      "0550 - Training acc: 0.8969597816467285\n",
      "0560 - Training acc: 0.839273989200592\n",
      "0570 - Training acc: 0.9239315390586853\n",
      "0580 - Training acc: 0.9153234362602234\n",
      "0590 - Training acc: 0.8147351741790771\n",
      "0600 - Training acc: 0.941135823726654\n",
      "0610 - Training acc: 0.925440788269043\n",
      "0620 - Training acc: 0.8988475203514099\n",
      "0630 - Training acc: 0.8977739214897156\n",
      "0640 - Training acc: 0.9351725578308105\n",
      "0650 - Training acc: 0.8851974010467529\n",
      "0660 - Training acc: 0.9286831021308899\n",
      "0670 - Training acc: 0.9235886931419373\n",
      "0680 - Training acc: 0.9392527937889099\n",
      "0690 - Training acc: 0.8069816827774048\n",
      "0700 - Training acc: 0.9074152112007141\n",
      "0710 - Training acc: 0.9066248536109924\n",
      "0720 - Training acc: 0.9349035024642944\n",
      "0730 - Training acc: 0.8913320899009705\n",
      "0740 - Training acc: 0.9159685373306274\n",
      "0750 - Training acc: 0.9310184717178345\n",
      "0760 - Training acc: 0.9275404810905457\n",
      "0770 - Training acc: 0.898868978023529\n",
      "0780 - Training acc: 0.9365294575691223\n",
      "0790 - Training acc: 0.8650221228599548\n",
      "0800 - Training acc: 0.9189727902412415\n",
      "0810 - Training acc: 0.9291639924049377\n",
      "0820 - Training acc: 0.9205415844917297\n",
      "0830 - Training acc: 0.9322349429130554\n",
      "0840 - Training acc: 0.9453160762786865\n",
      "0850 - Training acc: 0.6736512184143066\n",
      "0860 - Training acc: 0.9297686815261841\n",
      "0870 - Training acc: 0.9109741449356079\n",
      "0880 - Training acc: 0.9229364395141602\n",
      "0890 - Training acc: 0.9102194905281067\n",
      "0900 - Training acc: 0.9236410856246948\n",
      "0910 - Training acc: 0.9093601107597351\n",
      "0920 - Training acc: 0.9296044111251831\n",
      "0930 - Training acc: 0.8994402885437012\n",
      "0940 - Training acc: 0.9287069439888\n",
      "0950 - Training acc: 0.9377483129501343\n",
      "0960 - Training acc: 0.7364171147346497\n",
      "0970 - Training acc: 0.9263954162597656\n",
      "0980 - Training acc: 0.9229459762573242\n",
      "0990 - Training acc: 0.8383288979530334\n",
      "1000 - Training acc: 0.912683367729187\n",
      "1010 - Training acc: 0.9404264092445374\n",
      "1020 - Training acc: 0.9287973642349243\n",
      "1030 - Training acc: 0.9036967754364014\n",
      "1040 - Training acc: 0.9238672256469727\n",
      "1050 - Training acc: 0.8978405594825745\n",
      "1060 - Training acc: 0.9103028178215027\n",
      "1070 - Training acc: 0.9294544458389282\n",
      "1080 - Training acc: 0.9138712882995605\n",
      "1090 - Training acc: 0.8879802823066711\n",
      "1100 - Training acc: 0.9289544820785522\n",
      "1110 - Training acc: 0.9472038745880127\n",
      "1120 - Training acc: 0.8983500003814697\n",
      "1130 - Training acc: 0.9263240098953247\n",
      "1140 - Training acc: 0.8271378874778748\n",
      "1150 - Training acc: 0.9078817963600159\n",
      "1160 - Training acc: 0.9207153916358948\n",
      "1170 - Training acc: 0.8994998335838318\n",
      "1180 - Training acc: 0.9412500858306885\n",
      "1190 - Training acc: 0.9099457263946533\n",
      "1200 - Training acc: 0.9174778461456299\n",
      "1210 - Training acc: 0.9275071024894714\n",
      "1220 - Training acc: 0.8847903609275818\n",
      "1230 - Training acc: 0.9304328560829163\n",
      "1240 - Training acc: 0.9347773790359497\n",
      "1250 - Training acc: 0.7680523991584778\n",
      "1260 - Training acc: 0.8965883851051331\n",
      "1270 - Training acc: 0.9281260967254639\n",
      "1280 - Training acc: 0.9111407399177551\n",
      "1290 - Training acc: 0.9267334342002869\n",
      "1300 - Training acc: 0.9265525341033936\n",
      "1310 - Training acc: 0.8195010423660278\n",
      "1320 - Training acc: 0.9390052556991577\n",
      "1330 - Training acc: 0.917877733707428\n",
      "1340 - Training acc: 0.8281853199005127\n",
      "1350 - Training acc: 0.9317659735679626\n",
      "1360 - Training acc: 0.9138379693031311\n",
      "1370 - Training acc: 0.9445233941078186\n",
      "1380 - Training acc: 0.9138641357421875\n",
      "1390 - Training acc: 0.9128595590591431\n",
      "1400 - Training acc: 0.9022089242935181\n",
      "1410 - Training acc: 0.9060083031654358\n",
      "1420 - Training acc: 0.8649173974990845\n",
      "1430 - Training acc: 0.9449732899665833\n",
      "1440 - Training acc: 0.9373816847801208\n",
      "1450 - Training acc: 0.9348487854003906\n",
      "1460 - Training acc: 0.8348152041435242\n",
      "1470 - Training acc: 0.9380053877830505\n",
      "1480 - Training acc: 0.8518600463867188\n",
      "1490 - Training acc: 0.9353963136672974\n",
      "1500 - Training acc: 0.9280856251716614\n",
      "1510 - Training acc: 0.9005782008171082\n",
      "1520 - Training acc: 0.844963550567627\n",
      "1530 - Training acc: 0.9322491884231567\n",
      "1540 - Training acc: 0.8402000665664673\n",
      "1550 - Training acc: 0.9303733110427856\n",
      "1560 - Training acc: 0.9355676770210266\n",
      "1570 - Training acc: 0.9228078722953796\n",
      "1580 - Training acc: 0.9162089824676514\n",
      "1590 - Training acc: 0.9355438947677612\n",
      "1600 - Training acc: 0.9036467671394348\n",
      "1610 - Training acc: 0.9206082820892334\n",
      "1620 - Training acc: 0.9079103469848633\n",
      "1630 - Training acc: 0.8940078616142273\n",
      "1640 - Training acc: 0.9288354516029358\n",
      "1650 - Training acc: 0.9148187041282654\n",
      "1660 - Training acc: 0.9055202603340149\n",
      "1670 - Training acc: 0.946744441986084\n",
      "1680 - Training acc: 0.927554726600647\n",
      "1690 - Training acc: 0.9302209615707397\n",
      "1700 - Training acc: 0.9423380494117737\n",
      "1710 - Training acc: 0.9276095032691956\n",
      "1720 - Training acc: 0.9100790619850159\n",
      "1730 - Training acc: 0.9286355376243591\n",
      "1740 - Training acc: 0.8889134526252747\n",
      "1750 - Training acc: 0.9489988088607788\n",
      "1760 - Training acc: 0.8868447542190552\n",
      "1770 - Training acc: 0.8695619106292725\n",
      "1780 - Training acc: 0.9386862516403198\n",
      "1790 - Training acc: 0.8722280859947205\n",
      "1800 - Training acc: 0.923407793045044\n",
      "1810 - Training acc: 0.862520158290863\n",
      "1820 - Training acc: 0.9195417761802673\n",
      "1830 - Training acc: 0.8375266790390015\n",
      "1840 - Training acc: 0.9201059341430664\n",
      "1850 - Training acc: 0.9383862614631653\n",
      "1860 - Training acc: 0.8731470108032227\n",
      "1870 - Training acc: 0.9278284907341003\n",
      "1880 - Training acc: 0.928859293460846\n",
      "1890 - Training acc: 0.902106523513794\n",
      "1900 - Training acc: 0.9052774310112\n",
      "1910 - Training acc: 0.8892824649810791\n",
      "1920 - Training acc: 0.9204273223876953\n",
      "1930 - Training acc: 0.9214366674423218\n",
      "1940 - Training acc: 0.9084221720695496\n",
      "1950 - Training acc: 0.9322944283485413\n",
      "1960 - Training acc: 0.860815703868866\n",
      "1970 - Training acc: 0.8626701831817627\n",
      "1980 - Training acc: 0.7973475456237793\n",
      "1990 - Training acc: 0.940412163734436\n",
      "2000 - Training acc: 0.9488750696182251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010 - Training acc: 0.9419428706169128\n",
      "2020 - Training acc: 0.9343345761299133\n",
      "2030 - Training acc: 0.9177420735359192\n",
      "2040 - Training acc: 0.8847332000732422\n",
      "2050 - Training acc: 0.9381054043769836\n",
      "2060 - Training acc: 0.9333323836326599\n",
      "2070 - Training acc: 0.8790150880813599\n",
      "2080 - Training acc: 0.9307804107666016\n",
      "2090 - Training acc: 0.9335132837295532\n",
      "2100 - Training acc: 0.9267786741256714\n",
      "2110 - Training acc: 0.9322039484977722\n",
      "2120 - Training acc: 0.9316278696060181\n",
      "2130 - Training acc: 0.9207963347434998\n",
      "2140 - Training acc: 0.9290211796760559\n",
      "2150 - Training acc: 0.9229221343994141\n",
      "2160 - Training acc: 0.9144212007522583\n",
      "2170 - Training acc: 0.9295544028282166\n",
      "2180 - Training acc: 0.9101195335388184\n",
      "2190 - Training acc: 0.8983952403068542\n",
      "2200 - Training acc: 0.9292734861373901\n",
      "2210 - Training acc: 0.908705472946167\n",
      "2220 - Training acc: 0.9147306680679321\n",
      "2230 - Training acc: 0.9398146271705627\n",
      "2240 - Training acc: 0.9146473407745361\n",
      "2250 - Training acc: 0.9491678476333618\n",
      "2260 - Training acc: 0.8954504728317261\n",
      "2270 - Training acc: 0.920415461063385\n",
      "2280 - Training acc: 0.9244195222854614\n",
      "2290 - Training acc: 0.8289352059364319\n",
      "2300 - Training acc: 0.932756245136261\n",
      "2310 - Training acc: 0.9422523379325867\n",
      "2320 - Training acc: 0.9092006087303162\n",
      "2330 - Training acc: 0.603036642074585\n",
      "2340 - Training acc: 0.9132737517356873\n",
      "2350 - Training acc: 0.9059916138648987\n",
      "2360 - Training acc: 0.9306709170341492\n",
      "2370 - Training acc: 0.9026207327842712\n",
      "2380 - Training acc: 0.9448542594909668\n",
      "2390 - Training acc: 0.9015137553215027\n",
      "2400 - Training acc: 0.9331942796707153\n",
      "2410 - Training acc: 0.8906798362731934\n",
      "2420 - Training acc: 0.9235220551490784\n",
      "2430 - Training acc: 0.9301947951316833\n",
      "2440 - Training acc: 0.9441543817520142\n",
      "2450 - Training acc: 0.9098528623580933\n",
      "2460 - Training acc: 0.9299495816230774\n",
      "2470 - Training acc: 0.9243433475494385\n",
      "2480 - Training acc: 0.9277094602584839\n",
      "2490 - Training acc: 0.8777367472648621\n",
      "2500 - Training acc: 0.9418761730194092\n",
      "2510 - Training acc: 0.9027159810066223\n",
      "2520 - Training acc: 0.9127857089042664\n",
      "2530 - Training acc: 0.939098060131073\n",
      "2540 - Training acc: 0.6290109157562256\n",
      "2550 - Training acc: 0.873506486415863\n",
      "2560 - Training acc: 0.9386386275291443\n",
      "2570 - Training acc: 0.8896704912185669\n",
      "2580 - Training acc: 0.9476228952407837\n",
      "2590 - Training acc: 0.9354463219642639\n",
      "2600 - Training acc: 0.8946791887283325\n",
      "2610 - Training acc: 0.9286283850669861\n",
      "2620 - Training acc: 0.9374769330024719\n",
      "2630 - Training acc: 0.9074913859367371\n",
      "2640 - Training acc: 0.8184393048286438\n",
      "2650 - Training acc: 0.919268012046814\n",
      "2660 - Training acc: 0.9012804627418518\n",
      "2670 - Training acc: 0.926333487033844\n",
      "2680 - Training acc: 0.9268381595611572\n",
      "2690 - Training acc: 0.9123191237449646\n",
      "2700 - Training acc: 0.9324086904525757\n",
      "2710 - Training acc: 0.9119906425476074\n",
      "2720 - Training acc: 0.9185371398925781\n",
      "2730 - Training acc: 0.9439282417297363\n",
      "2740 - Training acc: 0.9235125184059143\n",
      "2750 - Training acc: 0.9354915022850037\n",
      "2760 - Training acc: 0.8992522358894348\n",
      "2770 - Training acc: 0.9037205576896667\n",
      "2780 - Training acc: 0.9174968600273132\n",
      "2790 - Training acc: 0.9261930584907532\n",
      "2800 - Training acc: 0.9002116322517395\n",
      "2810 - Training acc: 0.928618848323822\n",
      "2820 - Training acc: 0.8947268128395081\n",
      "2830 - Training acc: 0.9431354999542236\n",
      "2840 - Training acc: 0.924298107624054\n",
      "2850 - Training acc: 0.9016256332397461\n",
      "2860 - Training acc: 0.9177706241607666\n",
      "2870 - Training acc: 0.9414024353027344\n",
      "2880 - Training acc: 0.9221175312995911\n",
      "2890 - Training acc: 0.8966003060340881\n",
      "2900 - Training acc: 0.9315540790557861\n",
      "2910 - Training acc: 0.939186155796051\n",
      "2920 - Training acc: 0.9088292121887207\n",
      "2930 - Training acc: 0.9242528676986694\n",
      "2940 - Training acc: 0.9181943535804749\n",
      "2950 - Training acc: 0.8554261326789856\n",
      "2960 - Training acc: 0.7316297888755798\n",
      "2970 - Training acc: 0.9106646776199341\n",
      "2980 - Training acc: 0.9149139523506165\n",
      "2990 - Training acc: 0.9449495077133179\n",
      "3000 - Training acc: 0.8644484281539917\n",
      "3010 - Training acc: 0.9323920607566833\n",
      "3020 - Training acc: 0.8732898235321045\n",
      "3030 - Training acc: 0.9476609826087952\n",
      "3040 - Training acc: 0.9121739268302917\n",
      "3050 - Training acc: 0.9312565326690674\n",
      "3060 - Training acc: 0.6775648593902588\n",
      "3070 - Training acc: 0.8997259736061096\n",
      "3080 - Training acc: 0.9482775330543518\n",
      "3090 - Training acc: 0.9444472193717957\n",
      "3100 - Training acc: 0.9426689147949219\n",
      "3110 - Training acc: 0.894879162311554\n",
      "3120 - Training acc: 0.8081767559051514\n",
      "3130 - Training acc: 0.9032372832298279\n",
      "3140 - Training acc: 0.9302923679351807\n",
      "3150 - Training acc: 0.9310517907142639\n",
      "3160 - Training acc: 0.9079674482345581\n",
      "3170 - Training acc: 0.8862496018409729\n",
      "3180 - Training acc: 0.9389480948448181\n",
      "3190 - Training acc: 0.9142997860908508\n",
      "3200 - Training acc: 0.9224269986152649\n",
      "3210 - Training acc: 0.8387264609336853\n",
      "3220 - Training acc: 0.9408025741577148\n",
      "3230 - Training acc: 0.9201202392578125\n",
      "3240 - Training acc: 0.9154329299926758\n",
      "3250 - Training acc: 0.9502748250961304\n",
      "3260 - Training acc: 0.9361890554428101\n",
      "3270 - Training acc: 0.9351915717124939\n",
      "3280 - Training acc: 0.9132713675498962\n",
      "3290 - Training acc: 0.8712378144264221\n",
      "3300 - Training acc: 0.9381768107414246\n",
      "3310 - Training acc: 0.9205463528633118\n",
      "3320 - Training acc: 0.8983428478240967\n",
      "3330 - Training acc: 0.8957409262657166\n",
      "3340 - Training acc: 0.9110860228538513\n",
      "3350 - Training acc: 0.806512713432312\n",
      "3360 - Training acc: 0.7017894387245178\n",
      "3370 - Training acc: 0.9481394290924072\n",
      "3380 - Training acc: 0.9317207336425781\n",
      "3390 - Training acc: 0.8800982236862183\n",
      "3400 - Training acc: 0.9252218008041382\n",
      "3410 - Training acc: 0.820415198802948\n",
      "3420 - Training acc: 0.9285640716552734\n",
      "3430 - Training acc: 0.8480273485183716\n",
      "3440 - Training acc: 0.9346773624420166\n",
      "3450 - Training acc: 0.8812694549560547\n",
      "3460 - Training acc: 0.9296901226043701\n",
      "3470 - Training acc: 0.9251337051391602\n",
      "3480 - Training acc: 0.9123810529708862\n",
      "3490 - Training acc: 0.924717128276825\n",
      "3500 - Training acc: 0.938298225402832\n",
      "3510 - Training acc: 0.6348956823348999\n",
      "3520 - Training acc: 0.9477347731590271\n",
      "3530 - Training acc: 0.8988070487976074\n",
      "3540 - Training acc: 0.9015423655509949\n",
      "3550 - Training acc: 0.8868162035942078\n",
      "3560 - Training acc: 0.9097124338150024\n",
      "3570 - Training acc: 0.8873803615570068\n",
      "3580 - Training acc: 0.9332347512245178\n",
      "3590 - Training acc: 0.9040419459342957\n",
      "3600 - Training acc: 0.9374507069587708\n",
      "3610 - Training acc: 0.9255145788192749\n",
      "3620 - Training acc: 0.9155519604682922\n",
      "3630 - Training acc: 0.8969716429710388\n",
      "3640 - Training acc: 0.942597508430481\n",
      "3650 - Training acc: 0.9207463264465332\n",
      "3660 - Training acc: 0.9039300680160522\n",
      "3670 - Training acc: 0.9017565846443176\n",
      "3680 - Training acc: 0.9084174036979675\n",
      "3690 - Training acc: 0.9123905301094055\n",
      "3700 - Training acc: 0.785537600517273\n",
      "3710 - Training acc: 0.9093101024627686\n",
      "3720 - Training acc: 0.8957980275154114\n",
      "3730 - Training acc: 0.9388076663017273\n",
      "3740 - Training acc: 0.9355915188789368\n",
      "3750 - Training acc: 0.894802987575531\n",
      "3760 - Training acc: 0.9341750741004944\n",
      "3770 - Training acc: 0.9177539348602295\n",
      "3780 - Training acc: 0.9089601635932922\n",
      "3790 - Training acc: 0.9016590118408203\n",
      "3800 - Training acc: 0.9316826462745667\n",
      "3810 - Training acc: 0.9334989786148071\n",
      "3820 - Training acc: 0.9089982509613037\n",
      "3830 - Training acc: 0.9417142868041992\n",
      "3840 - Training acc: 0.9381054043769836\n",
      "3850 - Training acc: 0.9262596964836121\n",
      "3860 - Training acc: 0.9269976615905762\n",
      "3870 - Training acc: 0.9415881633758545\n",
      "3880 - Training acc: 0.9481418132781982\n",
      "3890 - Training acc: 0.9170159697532654\n",
      "3900 - Training acc: 0.9192013740539551\n",
      "3910 - Training acc: 0.9240434169769287\n",
      "3920 - Training acc: 0.9428950548171997\n",
      "3930 - Training acc: 0.8686953783035278\n",
      "3940 - Training acc: 0.9358152747154236\n",
      "3950 - Training acc: 0.9153567552566528\n",
      "3960 - Training acc: 0.921222448348999\n",
      "3970 - Training acc: 0.8934365510940552\n",
      "3980 - Training acc: 0.9464516043663025\n",
      "3990 - Training acc: 0.9398336410522461\n",
      "4000 - Training acc: 0.8229743242263794\n",
      "4010 - Training acc: 0.9227579236030579\n",
      "4020 - Training acc: 0.9307184815406799\n",
      "4030 - Training acc: 0.9135237336158752\n",
      "4040 - Training acc: 0.9029468894004822\n",
      "4050 - Training acc: 0.9440591931343079\n",
      "4060 - Training acc: 0.9409715533256531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4070 - Training acc: 0.9294663071632385\n",
      "4080 - Training acc: 0.9130499958992004\n",
      "4090 - Training acc: 0.9080103039741516\n",
      "4100 - Training acc: 0.8685644268989563\n",
      "4110 - Training acc: 0.9384338855743408\n",
      "4120 - Training acc: 0.9462540149688721\n",
      "4130 - Training acc: 0.8806672096252441\n",
      "4140 - Training acc: 0.9086911678314209\n",
      "4150 - Training acc: 0.9364223480224609\n",
      "4160 - Training acc: 0.8962075114250183\n",
      "4170 - Training acc: 0.9198464751243591\n",
      "4180 - Training acc: 0.9337013363838196\n",
      "4190 - Training acc: 0.9454065561294556\n",
      "4200 - Training acc: 0.9467611312866211\n",
      "4210 - Training acc: 0.9444257616996765\n",
      "4220 - Training acc: 0.9362009167671204\n",
      "4230 - Training acc: 0.8982857465744019\n",
      "4240 - Training acc: 0.927397608757019\n",
      "4250 - Training acc: 0.9412263035774231\n",
      "4260 - Training acc: 0.9201107025146484\n",
      "4270 - Training acc: 0.9321396946907043\n",
      "4280 - Training acc: 0.9375959634780884\n",
      "4290 - Training acc: 0.9164375066757202\n",
      "4300 - Training acc: 0.9028754830360413\n",
      "4310 - Training acc: 0.9263620972633362\n",
      "4320 - Training acc: 0.9260883331298828\n",
      "4330 - Training acc: 0.8016825318336487\n",
      "4340 - Training acc: 0.9475752711296082\n",
      "4350 - Training acc: 0.9405525922775269\n",
      "4360 - Training acc: 0.9417833685874939\n",
      "4370 - Training acc: 0.932761013507843\n",
      "4380 - Training acc: 0.890575110912323\n",
      "4390 - Training acc: 0.9295448660850525\n",
      "4400 - Training acc: 0.9317612051963806\n",
      "4410 - Training acc: 0.92316734790802\n",
      "4420 - Training acc: 0.8885373473167419\n",
      "4430 - Training acc: 0.9289116263389587\n",
      "4440 - Training acc: 0.9194727540016174\n",
      "4450 - Training acc: 0.9237148761749268\n",
      "4460 - Training acc: 0.9320706725120544\n",
      "4470 - Training acc: 0.8602753281593323\n",
      "4480 - Training acc: 0.9341941475868225\n",
      "4490 - Training acc: 0.9252575039863586\n",
      "4500 - Training acc: 0.910650372505188\n",
      "4510 - Training acc: 0.8536121249198914\n",
      "4520 - Training acc: 0.8946577310562134\n",
      "4530 - Training acc: 0.9087435603141785\n",
      "4540 - Training acc: 0.9411191940307617\n",
      "4550 - Training acc: 0.8924938440322876\n",
      "4560 - Training acc: 0.9343393445014954\n",
      "4570 - Training acc: 0.9079365134239197\n",
      "4580 - Training acc: 0.922103226184845\n",
      "4590 - Training acc: 0.9483346343040466\n",
      "4600 - Training acc: 0.9435354471206665\n",
      "4610 - Training acc: 0.9451137781143188\n",
      "4620 - Training acc: 0.9354653358459473\n",
      "4630 - Training acc: 0.9283760190010071\n",
      "4640 - Training acc: 0.9304590225219727\n",
      "4650 - Training acc: 0.9475204944610596\n",
      "4660 - Training acc: 0.9463254809379578\n",
      "4670 - Training acc: 0.9119620323181152\n",
      "4680 - Training acc: 0.9032729864120483\n",
      "4690 - Training acc: 0.9325348734855652\n",
      "4700 - Training acc: 0.9315754771232605\n",
      "4710 - Training acc: 0.9207630157470703\n",
      "4720 - Training acc: 0.9450827836990356\n",
      "4730 - Training acc: 0.9489559531211853\n",
      "4740 - Training acc: 0.9484703540802002\n",
      "4750 - Training acc: 0.9184300303459167\n",
      "4760 - Training acc: 0.9005353450775146\n",
      "4770 - Training acc: 0.9127857089042664\n",
      "4780 - Training acc: 0.9207463264465332\n",
      "4790 - Training acc: 0.9187394976615906\n",
      "4800 - Training acc: 0.921470046043396\n",
      "4810 - Training acc: 0.9136070013046265\n",
      "4820 - Training acc: 0.9098909497261047\n",
      "4830 - Training acc: 0.9217889904975891\n",
      "4840 - Training acc: 0.9478442668914795\n",
      "4850 - Training acc: 0.9284902811050415\n",
      "4860 - Training acc: 0.9249027967453003\n",
      "4870 - Training acc: 0.92934250831604\n",
      "4880 - Training acc: 0.919101357460022\n",
      "4890 - Training acc: 0.8998497724533081\n",
      "4900 - Training acc: 0.9041538238525391\n",
      "4910 - Training acc: 0.9319087862968445\n",
      "4920 - Training acc: 0.9192917943000793\n",
      "4930 - Training acc: 0.9540122747421265\n",
      "4940 - Training acc: 0.9307613372802734\n",
      "4950 - Training acc: 0.9477585554122925\n",
      "4960 - Training acc: 0.9018827676773071\n",
      "4970 - Training acc: 0.9454851150512695\n",
      "4980 - Training acc: 0.8704331517219543\n",
      "4990 - Training acc: 0.9342893362045288\n",
      "5000 - Training acc: 0.9224079847335815\n",
      "<class 'torch.Tensor'>\n",
      "420069\n",
      "180029\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>probs_0</th>\n",
       "      <th>probs_1</th>\n",
       "      <th>gts</th>\n",
       "      <th>tvt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.998853</td>\n",
       "      <td>1.146895e-03</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.994569</td>\n",
       "      <td>5.430795e-03</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.999999</td>\n",
       "      <td>7.975501e-07</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.994542</td>\n",
       "      <td>5.458216e-03</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.983893</td>\n",
       "      <td>1.610726e-02</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180024</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180025</th>\n",
       "      <td>0.999744</td>\n",
       "      <td>2.565124e-04</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180026</th>\n",
       "      <td>0.935530</td>\n",
       "      <td>6.447051e-02</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180027</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.924949e-08</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180028</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.641669e-11</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600098 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         probs_0       probs_1  gts    tvt\n",
       "0       0.998853  1.146895e-03    0  train\n",
       "1       0.994569  5.430795e-03    0  train\n",
       "2       0.999999  7.975501e-07    0  train\n",
       "3       0.994542  5.458216e-03    0  train\n",
       "4       0.983893  1.610726e-02    0  train\n",
       "...          ...           ...  ...    ...\n",
       "180024  0.000000  1.000000e+00    1   test\n",
       "180025  0.999744  2.565124e-04    0   test\n",
       "180026  0.935530  6.447051e-02    0   test\n",
       "180027  1.000000  6.924949e-08    0   test\n",
       "180028  1.000000  2.641669e-11    0   test\n",
       "\n",
       "[600098 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cname_label = 'Label'\n",
    "bot_bin_prob_df = run_baseline(\n",
    "    ds_name,\n",
    "    cname_label,\n",
    "    n_epochs\n",
    ")\n",
    "bot_bin_prob_df\n",
    "bot_bin_prob_df.to_csv('../output/EGraphSAGE_nf_bot_binary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc997458",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "457a1655",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/miniconda2/envs/iot/lib/python3.9/site-packages/category_encoders/target_encoder.py:92: FutureWarning: Default parameter min_samples_leaf will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter min_samples_leaf will change in version 2.6.\"\n",
      "/home/hoang/miniconda2/envs/iot/lib/python3.9/site-packages/category_encoders/target_encoder.py:97: FutureWarning: Default parameter smoothing will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter smoothing will change in version 2.6.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FLOW_DURATION_MILLISECONDS', 'PROTOCOL', 'L7_PROTO', 'OUT_BYTES', 'IN_BYTES', 'TCP_FLAGS', 'IN_PKTS', 'OUT_PKTS']\n",
      "['FLOW_DURATION_MILLISECONDS', 'PROTOCOL', 'L7_PROTO', 'OUT_BYTES', 'IN_BYTES', 'TCP_FLAGS', 'IN_PKTS', 'OUT_PKTS']\n",
      "0010 - Training acc: 0.09482274204492569\n",
      "0020 - Training acc: 0.09626536816358566\n",
      "0030 - Training acc: 0.09626536816358566\n",
      "0040 - Training acc: 0.09650342166423798\n",
      "0050 - Training acc: 0.6532894372940063\n",
      "0060 - Training acc: 0.7228424549102783\n",
      "0070 - Training acc: 0.7339621186256409\n",
      "0080 - Training acc: 0.7535398602485657\n",
      "0090 - Training acc: 0.75748211145401\n",
      "0100 - Training acc: 0.7581415176391602\n",
      "0110 - Training acc: 0.7686017155647278\n",
      "0120 - Training acc: 0.7674209475517273\n",
      "0130 - Training acc: 0.7795285582542419\n",
      "0140 - Training acc: 0.7741317749023438\n",
      "0150 - Training acc: 0.7720106840133667\n",
      "0160 - Training acc: 0.7971494793891907\n",
      "0170 - Training acc: 0.7939381003379822\n",
      "0180 - Training acc: 0.8026081323623657\n",
      "0190 - Training acc: 0.8043578267097473\n",
      "0200 - Training acc: 0.8063194155693054\n",
      "0210 - Training acc: 0.8028342723846436\n",
      "0220 - Training acc: 0.7998990416526794\n",
      "0230 - Training acc: 0.8016606569290161\n",
      "0240 - Training acc: 0.8063099384307861\n",
      "0250 - Training acc: 0.8081953525543213\n",
      "0260 - Training acc: 0.8090380430221558\n",
      "0270 - Training acc: 0.806121826171875\n",
      "0280 - Training acc: 0.8049434423446655\n",
      "0290 - Training acc: 0.8082286715507507\n",
      "0300 - Training acc: 0.8088713884353638\n",
      "0310 - Training acc: 0.8123970031738281\n",
      "0320 - Training acc: 0.8113543391227722\n",
      "0330 - Training acc: 0.810606837272644\n",
      "0340 - Training acc: 0.8158535957336426\n",
      "0350 - Training acc: 0.8093403577804565\n",
      "0360 - Training acc: 0.8159440755844116\n",
      "0370 - Training acc: 0.8146942853927612\n",
      "0380 - Training acc: 0.8135063648223877\n",
      "0390 - Training acc: 0.8182627558708191\n",
      "0400 - Training acc: 0.8150299191474915\n",
      "0410 - Training acc: 0.817112922668457\n",
      "0420 - Training acc: 0.8188816905021667\n",
      "0430 - Training acc: 0.8092641830444336\n",
      "0440 - Training acc: 0.8194316029548645\n",
      "0450 - Training acc: 0.8210480213165283\n",
      "0460 - Training acc: 0.8255163431167603\n",
      "0470 - Training acc: 0.829063355922699\n",
      "0480 - Training acc: 0.813942015171051\n",
      "0490 - Training acc: 0.8255972862243652\n",
      "0500 - Training acc: 0.8250330686569214\n",
      "0510 - Training acc: 0.8276088237762451\n",
      "0520 - Training acc: 0.8086143136024475\n",
      "0530 - Training acc: 0.8237214088439941\n",
      "0540 - Training acc: 0.8029604554176331\n",
      "0550 - Training acc: 0.8220382928848267\n",
      "0560 - Training acc: 0.8277040719985962\n",
      "0570 - Training acc: 0.820700466632843\n",
      "0580 - Training acc: 0.8059528470039368\n",
      "0590 - Training acc: 0.8076525330543518\n",
      "0600 - Training acc: 0.8152203559875488\n",
      "0610 - Training acc: 0.8188793063163757\n",
      "0620 - Training acc: 0.8292276263237\n",
      "0630 - Training acc: 0.8310558795928955\n",
      "0640 - Training acc: 0.8221859335899353\n",
      "0650 - Training acc: 0.8104521036148071\n",
      "0660 - Training acc: 0.8260353207588196\n",
      "0670 - Training acc: 0.8297322988510132\n",
      "0680 - Training acc: 0.8199101090431213\n",
      "0690 - Training acc: 0.8161702156066895\n",
      "0700 - Training acc: 0.8218621611595154\n",
      "0710 - Training acc: 0.8303869962692261\n",
      "0720 - Training acc: 0.8277897834777832\n",
      "0730 - Training acc: 0.8161869049072266\n",
      "0740 - Training acc: 0.8066527247428894\n",
      "0750 - Training acc: 0.8296061158180237\n",
      "0760 - Training acc: 0.8263471722602844\n",
      "0770 - Training acc: 0.8306797742843628\n",
      "0780 - Training acc: 0.8231238722801208\n",
      "0790 - Training acc: 0.8241332173347473\n",
      "0800 - Training acc: 0.8286657929420471\n",
      "0810 - Training acc: 0.8155965209007263\n",
      "0820 - Training acc: 0.8184294104576111\n",
      "0830 - Training acc: 0.8145895004272461\n",
      "0840 - Training acc: 0.8177104592323303\n",
      "0850 - Training acc: 0.809376060962677\n",
      "0860 - Training acc: 0.815075159072876\n",
      "0870 - Training acc: 0.8108020424842834\n",
      "0880 - Training acc: 0.8226025104522705\n",
      "0890 - Training acc: 0.8300870060920715\n",
      "0900 - Training acc: 0.8326675295829773\n",
      "0910 - Training acc: 0.8242665529251099\n",
      "0920 - Training acc: 0.8179389834403992\n",
      "0930 - Training acc: 0.8312368392944336\n",
      "0940 - Training acc: 0.8291609883308411\n",
      "0950 - Training acc: 0.8268446922302246\n",
      "0960 - Training acc: 0.8269160985946655\n",
      "0970 - Training acc: 0.8185341358184814\n",
      "0980 - Training acc: 0.8164297342300415\n",
      "0990 - Training acc: 0.8094475269317627\n",
      "1000 - Training acc: 0.8330103754997253\n",
      "1010 - Training acc: 0.8258829116821289\n",
      "1020 - Training acc: 0.81728196144104\n",
      "1030 - Training acc: 0.8195125460624695\n",
      "1040 - Training acc: 0.8233904838562012\n",
      "1050 - Training acc: 0.8231000304222107\n",
      "1060 - Training acc: 0.8297132849693298\n",
      "1070 - Training acc: 0.8275064826011658\n",
      "1080 - Training acc: 0.8306393027305603\n",
      "1090 - Training acc: 0.8275326490402222\n",
      "1100 - Training acc: 0.8280016183853149\n",
      "1110 - Training acc: 0.8263043165206909\n",
      "1120 - Training acc: 0.8220859169960022\n",
      "1130 - Training acc: 0.8237332701683044\n",
      "1140 - Training acc: 0.8320914506912231\n",
      "1150 - Training acc: 0.8259257674217224\n",
      "1160 - Training acc: 0.8188936114311218\n",
      "1170 - Training acc: 0.8324556946754456\n",
      "1180 - Training acc: 0.828006386756897\n",
      "1190 - Training acc: 0.8251878023147583\n",
      "1200 - Training acc: 0.826154351234436\n",
      "1210 - Training acc: 0.8175152540206909\n",
      "1220 - Training acc: 0.8274446129798889\n",
      "1230 - Training acc: 0.8269684910774231\n",
      "1240 - Training acc: 0.816463053226471\n",
      "1250 - Training acc: 0.8266732692718506\n",
      "1260 - Training acc: 0.8264066576957703\n",
      "1270 - Training acc: 0.8261209726333618\n",
      "1280 - Training acc: 0.8242260813713074\n",
      "1290 - Training acc: 0.8211384415626526\n",
      "1300 - Training acc: 0.8114733695983887\n",
      "1310 - Training acc: 0.8272446393966675\n",
      "1320 - Training acc: 0.8293300271034241\n",
      "1330 - Training acc: 0.8226048946380615\n",
      "1340 - Training acc: 0.8229215145111084\n",
      "1350 - Training acc: 0.8270875215530396\n",
      "1360 - Training acc: 0.8266327977180481\n",
      "1370 - Training acc: 0.8170510530471802\n",
      "1380 - Training acc: 0.8270303606987\n",
      "1390 - Training acc: 0.8345315456390381\n",
      "1400 - Training acc: 0.8324604630470276\n",
      "1410 - Training acc: 0.8226739168167114\n",
      "1420 - Training acc: 0.8284634947776794\n",
      "1430 - Training acc: 0.8337007164955139\n",
      "1440 - Training acc: 0.8234690427780151\n",
      "1450 - Training acc: 0.8331436514854431\n",
      "1460 - Training acc: 0.8347005844116211\n",
      "1470 - Training acc: 0.8255544304847717\n",
      "1480 - Training acc: 0.8313297033309937\n",
      "1490 - Training acc: 0.8338078260421753\n",
      "1500 - Training acc: 0.8301894068717957\n",
      "1510 - Training acc: 0.8187888264656067\n",
      "1520 - Training acc: 0.8271017670631409\n",
      "1530 - Training acc: 0.8340030312538147\n",
      "1540 - Training acc: 0.825985312461853\n",
      "1550 - Training acc: 0.8336983323097229\n",
      "1560 - Training acc: 0.8340054154396057\n",
      "1570 - Training acc: 0.8348647952079773\n",
      "1580 - Training acc: 0.8243212699890137\n",
      "1590 - Training acc: 0.8276088237762451\n",
      "1600 - Training acc: 0.8211836814880371\n",
      "1610 - Training acc: 0.8232119679450989\n",
      "1620 - Training acc: 0.8271470069885254\n",
      "1630 - Training acc: 0.8292728662490845\n",
      "1640 - Training acc: 0.8267685174942017\n",
      "1650 - Training acc: 0.8350409865379333\n",
      "1660 - Training acc: 0.8350886106491089\n",
      "1670 - Training acc: 0.8237975835800171\n",
      "1680 - Training acc: 0.8174652457237244\n",
      "1690 - Training acc: 0.8223073482513428\n",
      "1700 - Training acc: 0.8264662027359009\n",
      "1710 - Training acc: 0.8336435556411743\n",
      "1720 - Training acc: 0.8338292837142944\n",
      "1730 - Training acc: 0.8312416076660156\n",
      "1740 - Training acc: 0.8171176910400391\n",
      "1750 - Training acc: 0.8289753198623657\n",
      "1760 - Training acc: 0.8346124887466431\n",
      "1770 - Training acc: 0.8292609453201294\n",
      "1780 - Training acc: 0.8231619596481323\n",
      "1790 - Training acc: 0.8231381177902222\n",
      "1800 - Training acc: 0.831103503704071\n",
      "1810 - Training acc: 0.828427791595459\n",
      "1820 - Training acc: 0.8350195288658142\n",
      "1830 - Training acc: 0.8333103060722351\n",
      "1840 - Training acc: 0.8292609453201294\n",
      "1850 - Training acc: 0.8294132947921753\n",
      "1860 - Training acc: 0.8339197039604187\n",
      "1870 - Training acc: 0.8276302814483643\n",
      "1880 - Training acc: 0.8295680284500122\n",
      "1890 - Training acc: 0.8187650442123413\n",
      "1900 - Training acc: 0.8264780640602112\n",
      "1910 - Training acc: 0.8350980877876282\n",
      "1920 - Training acc: 0.8339292407035828\n",
      "1930 - Training acc: 0.8283206224441528\n",
      "1940 - Training acc: 0.8156631588935852\n",
      "1950 - Training acc: 0.8194553852081299\n",
      "1960 - Training acc: 0.8331364989280701\n",
      "1970 - Training acc: 0.8295894861221313\n",
      "1980 - Training acc: 0.8214360475540161\n",
      "1990 - Training acc: 0.8320819139480591\n",
      "2000 - Training acc: 0.8339030742645264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010 - Training acc: 0.8291729092597961\n",
      "2020 - Training acc: 0.8227667808532715\n",
      "2030 - Training acc: 0.8209980130195618\n",
      "2040 - Training acc: 0.8284658789634705\n",
      "2050 - Training acc: 0.834931492805481\n",
      "2060 - Training acc: 0.8285396695137024\n",
      "2070 - Training acc: 0.8311582803726196\n",
      "2080 - Training acc: 0.8258829116821289\n",
      "2090 - Training acc: 0.8332055807113647\n",
      "2100 - Training acc: 0.8274231553077698\n",
      "2110 - Training acc: 0.8301417827606201\n",
      "2120 - Training acc: 0.8340292572975159\n",
      "2130 - Training acc: 0.8295037746429443\n",
      "2140 - Training acc: 0.821500301361084\n",
      "2150 - Training acc: 0.831432044506073\n",
      "2160 - Training acc: 0.8289229273796082\n",
      "2170 - Training acc: 0.8341435194015503\n",
      "2180 - Training acc: 0.8296799659729004\n",
      "2190 - Training acc: 0.821747899055481\n",
      "2200 - Training acc: 0.8234618902206421\n",
      "2210 - Training acc: 0.8283896446228027\n",
      "2220 - Training acc: 0.8300489187240601\n",
      "2230 - Training acc: 0.8353456854820251\n",
      "2240 - Training acc: 0.8332031965255737\n",
      "2250 - Training acc: 0.8264876008033752\n",
      "2260 - Training acc: 0.8319891095161438\n",
      "2270 - Training acc: 0.8352100253105164\n",
      "2280 - Training acc: 0.8350028991699219\n",
      "2290 - Training acc: 0.8244426846504211\n",
      "2300 - Training acc: 0.828425407409668\n",
      "2310 - Training acc: 0.8189507126808167\n",
      "2320 - Training acc: 0.8298751711845398\n",
      "2330 - Training acc: 0.8351837992668152\n",
      "2340 - Training acc: 0.8280659317970276\n",
      "2350 - Training acc: 0.831436812877655\n",
      "2360 - Training acc: 0.8351814150810242\n",
      "2370 - Training acc: 0.8297204375267029\n",
      "2380 - Training acc: 0.8320890665054321\n",
      "2390 - Training acc: 0.822390615940094\n",
      "2400 - Training acc: 0.8260138630867004\n",
      "2410 - Training acc: 0.8277516961097717\n",
      "2420 - Training acc: 0.8321223855018616\n",
      "2430 - Training acc: 0.8327318429946899\n",
      "2440 - Training acc: 0.8249425888061523\n",
      "2450 - Training acc: 0.8352885842323303\n",
      "2460 - Training acc: 0.8339673280715942\n",
      "2470 - Training acc: 0.822478711605072\n",
      "2480 - Training acc: 0.8341006636619568\n",
      "2490 - Training acc: 0.8230714797973633\n",
      "2500 - Training acc: 0.832329511642456\n",
      "2510 - Training acc: 0.8239284753799438\n",
      "2520 - Training acc: 0.8150180578231812\n",
      "2530 - Training acc: 0.8334221839904785\n",
      "2540 - Training acc: 0.8224216103553772\n",
      "2550 - Training acc: 0.8228667378425598\n",
      "2560 - Training acc: 0.8260233998298645\n",
      "2570 - Training acc: 0.8217645287513733\n",
      "2580 - Training acc: 0.8304297924041748\n",
      "2590 - Training acc: 0.8352528214454651\n",
      "2600 - Training acc: 0.8297846913337708\n",
      "2610 - Training acc: 0.8226549029350281\n",
      "2620 - Training acc: 0.8278207182884216\n",
      "2630 - Training acc: 0.8296347260475159\n",
      "2640 - Training acc: 0.8285849094390869\n",
      "2650 - Training acc: 0.8349648118019104\n",
      "2660 - Training acc: 0.8278635740280151\n",
      "2670 - Training acc: 0.8334864974021912\n",
      "2680 - Training acc: 0.8213860392570496\n",
      "2690 - Training acc: 0.8236023783683777\n",
      "2700 - Training acc: 0.8270303606987\n",
      "2710 - Training acc: 0.8352647423744202\n",
      "2720 - Training acc: 0.834441065788269\n",
      "2730 - Training acc: 0.8342220783233643\n",
      "2740 - Training acc: 0.8297275304794312\n",
      "2750 - Training acc: 0.8268351554870605\n",
      "2760 - Training acc: 0.8323652148246765\n",
      "2770 - Training acc: 0.8225787281990051\n",
      "2780 - Training acc: 0.8278326392173767\n",
      "2790 - Training acc: 0.8286253213882446\n",
      "2800 - Training acc: 0.8353528380393982\n",
      "2810 - Training acc: 0.820779025554657\n",
      "2820 - Training acc: 0.8229548335075378\n",
      "2830 - Training acc: 0.8272565007209778\n",
      "2840 - Training acc: 0.8267280459403992\n",
      "2850 - Training acc: 0.834355354309082\n",
      "2860 - Training acc: 0.8354480266571045\n",
      "2870 - Training acc: 0.831186830997467\n",
      "2880 - Training acc: 0.8352623581886292\n",
      "2890 - Training acc: 0.8353123664855957\n",
      "2900 - Training acc: 0.826144814491272\n",
      "2910 - Training acc: 0.8222382664680481\n",
      "2920 - Training acc: 0.8224073052406311\n",
      "2930 - Training acc: 0.8288515210151672\n",
      "2940 - Training acc: 0.8352862000465393\n",
      "2950 - Training acc: 0.8337388038635254\n",
      "2960 - Training acc: 0.8293823599815369\n",
      "2970 - Training acc: 0.832236647605896\n",
      "2980 - Training acc: 0.7893626689910889\n",
      "2990 - Training acc: 0.79547119140625\n",
      "3000 - Training acc: 0.7983421683311462\n",
      "3010 - Training acc: 0.8093451261520386\n",
      "3020 - Training acc: 0.823945164680481\n",
      "3030 - Training acc: 0.8303155303001404\n",
      "3040 - Training acc: 0.830039381980896\n",
      "3050 - Training acc: 0.831020176410675\n",
      "3060 - Training acc: 0.8316415548324585\n",
      "3070 - Training acc: 0.832403302192688\n",
      "3080 - Training acc: 0.8296965956687927\n",
      "3090 - Training acc: 0.8299251198768616\n",
      "3100 - Training acc: 0.8244450688362122\n",
      "3110 - Training acc: 0.8296204209327698\n",
      "3120 - Training acc: 0.8205028772354126\n",
      "3130 - Training acc: 0.8300846219062805\n",
      "3140 - Training acc: 0.833626925945282\n",
      "3150 - Training acc: 0.8278707265853882\n",
      "3160 - Training acc: 0.8326699137687683\n",
      "3170 - Training acc: 0.8264328241348267\n",
      "3180 - Training acc: 0.8244022130966187\n",
      "3190 - Training acc: 0.8330222368240356\n",
      "3200 - Training acc: 0.8329817652702332\n",
      "3210 - Training acc: 0.833369791507721\n",
      "3220 - Training acc: 0.8285015821456909\n",
      "3230 - Training acc: 0.8318009972572327\n",
      "3240 - Training acc: 0.8227691650390625\n",
      "3250 - Training acc: 0.8295513987541199\n",
      "3260 - Training acc: 0.8342887163162231\n",
      "3270 - Training acc: 0.8272374868392944\n",
      "3280 - Training acc: 0.8286110758781433\n",
      "3290 - Training acc: 0.8304179310798645\n",
      "3300 - Training acc: 0.8351361751556396\n",
      "3310 - Training acc: 0.8350743055343628\n",
      "3320 - Training acc: 0.81613689661026\n",
      "3330 - Training acc: 0.821495532989502\n",
      "3340 - Training acc: 0.8265637755393982\n",
      "3350 - Training acc: 0.8229405283927917\n",
      "3360 - Training acc: 0.8226262927055359\n",
      "3370 - Training acc: 0.8289086222648621\n",
      "3380 - Training acc: 0.8259067535400391\n",
      "3390 - Training acc: 0.8258852958679199\n",
      "3400 - Training acc: 0.8338125944137573\n",
      "3410 - Training acc: 0.8223811388015747\n",
      "3420 - Training acc: 0.8203243017196655\n",
      "3430 - Training acc: 0.8353552222251892\n",
      "3440 - Training acc: 0.8317986726760864\n",
      "3450 - Training acc: 0.8267328143119812\n",
      "3460 - Training acc: 0.8272660374641418\n",
      "3470 - Training acc: 0.8350314497947693\n",
      "3480 - Training acc: 0.8340697288513184\n",
      "3490 - Training acc: 0.823949933052063\n",
      "3500 - Training acc: 0.8258686661720276\n",
      "3510 - Training acc: 0.8265613913536072\n",
      "3520 - Training acc: 0.8300179839134216\n",
      "3530 - Training acc: 0.8332769870758057\n",
      "3540 - Training acc: 0.8217359781265259\n",
      "3550 - Training acc: 0.8355171084403992\n",
      "3560 - Training acc: 0.8230500817298889\n",
      "3570 - Training acc: 0.8220025897026062\n",
      "3580 - Training acc: 0.8216907382011414\n",
      "3590 - Training acc: 0.8192530870437622\n",
      "3600 - Training acc: 0.8341363668441772\n",
      "3610 - Training acc: 0.8278635740280151\n",
      "3620 - Training acc: 0.8326199650764465\n",
      "3630 - Training acc: 0.8281516432762146\n",
      "3640 - Training acc: 0.8154917359352112\n",
      "3650 - Training acc: 0.8331079483032227\n",
      "3660 - Training acc: 0.8245855569839478\n",
      "3670 - Training acc: 0.8266542553901672\n",
      "3680 - Training acc: 0.82686847448349\n",
      "3690 - Training acc: 0.8349600434303284\n",
      "3700 - Training acc: 0.8221597075462341\n",
      "3710 - Training acc: 0.8276731371879578\n",
      "3720 - Training acc: 0.8360407948493958\n",
      "3730 - Training acc: 0.8305964469909668\n",
      "3740 - Training acc: 0.8251616358757019\n",
      "3750 - Training acc: 0.8300989270210266\n",
      "3760 - Training acc: 0.8265923261642456\n",
      "3770 - Training acc: 0.8342934846878052\n",
      "3780 - Training acc: 0.8199124932289124\n",
      "3790 - Training acc: 0.8255305886268616\n",
      "3800 - Training acc: 0.8350886106491089\n",
      "3810 - Training acc: 0.8354290127754211\n",
      "3820 - Training acc: 0.8240237236022949\n",
      "3830 - Training acc: 0.8207480311393738\n",
      "3840 - Training acc: 0.8257758021354675\n",
      "3850 - Training acc: 0.8273303508758545\n",
      "3860 - Training acc: 0.8219764232635498\n",
      "3870 - Training acc: 0.8288491368293762\n",
      "3880 - Training acc: 0.8204576373100281\n",
      "3890 - Training acc: 0.8227572441101074\n",
      "3900 - Training acc: 0.8193578124046326\n",
      "3910 - Training acc: 0.8303322196006775\n",
      "3920 - Training acc: 0.8212003707885742\n",
      "3930 - Training acc: 0.8239403963088989\n",
      "3940 - Training acc: 0.8332674503326416\n",
      "3950 - Training acc: 0.8314415812492371\n",
      "3960 - Training acc: 0.8293442726135254\n",
      "3970 - Training acc: 0.8210884928703308\n",
      "3980 - Training acc: 0.8320462107658386\n",
      "3990 - Training acc: 0.8352814316749573\n",
      "4000 - Training acc: 0.8277778625488281\n",
      "4010 - Training acc: 0.8256377577781677\n",
      "4020 - Training acc: 0.8353409171104431\n",
      "4030 - Training acc: 0.8351528644561768\n",
      "4040 - Training acc: 0.832817554473877\n",
      "4050 - Training acc: 0.8198720216751099\n",
      "4060 - Training acc: 0.8360908031463623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4070 - Training acc: 0.8285991549491882\n",
      "4080 - Training acc: 0.8360313177108765\n",
      "4090 - Training acc: 0.8204861879348755\n",
      "4100 - Training acc: 0.8205528259277344\n",
      "4110 - Training acc: 0.8212741613388062\n",
      "4120 - Training acc: 0.8148775696754456\n",
      "4130 - Training acc: 0.8336983323097229\n",
      "4140 - Training acc: 0.8278802037239075\n",
      "4150 - Training acc: 0.8288776874542236\n",
      "4160 - Training acc: 0.8328818082809448\n",
      "4170 - Training acc: 0.8282325267791748\n",
      "4180 - Training acc: 0.8356337547302246\n",
      "4190 - Training acc: 0.8304060101509094\n",
      "4200 - Training acc: 0.8260138630867004\n",
      "4210 - Training acc: 0.8205171227455139\n",
      "4220 - Training acc: 0.8366811871528625\n",
      "4230 - Training acc: 0.835831344127655\n",
      "4240 - Training acc: 0.8286253213882446\n",
      "4250 - Training acc: 0.8303131461143494\n",
      "4260 - Training acc: 0.825092613697052\n",
      "4270 - Training acc: 0.8323937654495239\n",
      "4280 - Training acc: 0.8360955715179443\n",
      "4290 - Training acc: 0.8354218602180481\n",
      "4300 - Training acc: 0.8286967873573303\n",
      "4310 - Training acc: 0.8277040719985962\n",
      "4320 - Training acc: 0.8227929472923279\n",
      "4330 - Training acc: 0.822804868221283\n",
      "4340 - Training acc: 0.8279469013214111\n",
      "4350 - Training acc: 0.8283896446228027\n",
      "4360 - Training acc: 0.8285991549491882\n",
      "4370 - Training acc: 0.835993230342865\n",
      "4380 - Training acc: 0.8339220881462097\n",
      "4390 - Training acc: 0.834769606590271\n",
      "4400 - Training acc: 0.8304345607757568\n",
      "4410 - Training acc: 0.8281301856040955\n",
      "4420 - Training acc: 0.8364335894584656\n",
      "4430 - Training acc: 0.8365859389305115\n",
      "4440 - Training acc: 0.8271684646606445\n",
      "4450 - Training acc: 0.8272803425788879\n",
      "4460 - Training acc: 0.8348219394683838\n",
      "4470 - Training acc: 0.8365288376808167\n",
      "4480 - Training acc: 0.8306416869163513\n",
      "4490 - Training acc: 0.8207218647003174\n",
      "4500 - Training acc: 0.8265542387962341\n",
      "4510 - Training acc: 0.8356409072875977\n",
      "4520 - Training acc: 0.821424126625061\n",
      "4530 - Training acc: 0.8223263621330261\n",
      "4540 - Training acc: 0.8357337117195129\n",
      "4550 - Training acc: 0.8368145227432251\n",
      "4560 - Training acc: 0.8366859555244446\n",
      "4570 - Training acc: 0.8361169695854187\n",
      "4580 - Training acc: 0.8238951563835144\n",
      "4590 - Training acc: 0.8288182020187378\n",
      "4600 - Training acc: 0.822642982006073\n",
      "4610 - Training acc: 0.8292585611343384\n",
      "4620 - Training acc: 0.8267780542373657\n",
      "4630 - Training acc: 0.8193411231040955\n",
      "4640 - Training acc: 0.8286824822425842\n",
      "4650 - Training acc: 0.823540449142456\n",
      "4660 - Training acc: 0.8356266021728516\n",
      "4670 - Training acc: 0.8275231719017029\n",
      "4680 - Training acc: 0.8208480477333069\n",
      "4690 - Training acc: 0.8229191303253174\n",
      "4700 - Training acc: 0.8214455842971802\n",
      "4710 - Training acc: 0.8367859125137329\n",
      "4720 - Training acc: 0.8285491466522217\n",
      "4730 - Training acc: 0.8300560712814331\n",
      "4740 - Training acc: 0.8327460885047913\n",
      "4750 - Training acc: 0.8276755213737488\n",
      "4760 - Training acc: 0.8347291350364685\n",
      "4770 - Training acc: 0.836719274520874\n",
      "4780 - Training acc: 0.8312439918518066\n",
      "4790 - Training acc: 0.8289419412612915\n",
      "4800 - Training acc: 0.8311630487442017\n",
      "4810 - Training acc: 0.8221692442893982\n",
      "4820 - Training acc: 0.8245998024940491\n",
      "4830 - Training acc: 0.8230238556861877\n",
      "4840 - Training acc: 0.8219883441925049\n",
      "4850 - Training acc: 0.8361051082611084\n",
      "4860 - Training acc: 0.8368097543716431\n",
      "4870 - Training acc: 0.834769606590271\n",
      "4880 - Training acc: 0.836707353591919\n",
      "4890 - Training acc: 0.8356004357337952\n",
      "4900 - Training acc: 0.8301537036895752\n",
      "4910 - Training acc: 0.8240498900413513\n",
      "4920 - Training acc: 0.8223430514335632\n",
      "4930 - Training acc: 0.8337554931640625\n",
      "4940 - Training acc: 0.824754536151886\n",
      "4950 - Training acc: 0.8246141076087952\n",
      "4960 - Training acc: 0.829318106174469\n",
      "4970 - Training acc: 0.8278897404670715\n",
      "4980 - Training acc: 0.8298656344413757\n",
      "4990 - Training acc: 0.8239737153053284\n",
      "5000 - Training acc: 0.8260971903800964\n",
      "<class 'torch.Tensor'>\n",
      "420068\n",
      "180030\n"
     ]
    }
   ],
   "source": [
    "cname_label = 'Attack'\n",
    "bot_multi_prob_df = run_baseline(\n",
    "    ds_name,\n",
    "    cname_label,\n",
    "    n_epochs\n",
    ")\n",
    "bot_multi_prob_df.to_csv('../output/EGraphSAGE_nf_bot_multi.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb02f2da",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>probs_0</th>\n",
       "      <th>probs_1</th>\n",
       "      <th>probs_2</th>\n",
       "      <th>probs_3</th>\n",
       "      <th>probs_4</th>\n",
       "      <th>gts</th>\n",
       "      <th>tvt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.704554</td>\n",
       "      <td>2.619193e-01</td>\n",
       "      <td>1.086907e-02</td>\n",
       "      <td>2.244496e-02</td>\n",
       "      <td>2.123700e-04</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.517022</td>\n",
       "      <td>4.539822e-01</td>\n",
       "      <td>1.341542e-02</td>\n",
       "      <td>1.558069e-02</td>\n",
       "      <td>3.908807e-09</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.513929</td>\n",
       "      <td>4.565644e-01</td>\n",
       "      <td>1.373505e-02</td>\n",
       "      <td>1.577194e-02</td>\n",
       "      <td>3.961761e-09</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.165381</td>\n",
       "      <td>7.686258e-01</td>\n",
       "      <td>3.182042e-02</td>\n",
       "      <td>3.212402e-02</td>\n",
       "      <td>2.048445e-03</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180025</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180026</th>\n",
       "      <td>0.919621</td>\n",
       "      <td>6.639876e-02</td>\n",
       "      <td>1.197097e-03</td>\n",
       "      <td>1.278253e-02</td>\n",
       "      <td>5.427218e-07</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180027</th>\n",
       "      <td>0.143455</td>\n",
       "      <td>7.962033e-01</td>\n",
       "      <td>3.180051e-02</td>\n",
       "      <td>2.622961e-02</td>\n",
       "      <td>2.312066e-03</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180028</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.190462e-40</td>\n",
       "      <td>6.458865e-41</td>\n",
       "      <td>5.076800e-08</td>\n",
       "      <td>2.220121e-30</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180029</th>\n",
       "      <td>0.999996</td>\n",
       "      <td>1.855692e-07</td>\n",
       "      <td>4.661761e-09</td>\n",
       "      <td>3.851798e-06</td>\n",
       "      <td>2.728606e-09</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600098 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         probs_0       probs_1       probs_2       probs_3       probs_4  gts  \\\n",
       "0       0.704554  2.619193e-01  1.086907e-02  2.244496e-02  2.123700e-04    0   \n",
       "1       0.517022  4.539822e-01  1.341542e-02  1.558069e-02  3.908807e-09    0   \n",
       "2       1.000000  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00    0   \n",
       "3       0.513929  4.565644e-01  1.373505e-02  1.577194e-02  3.961761e-09    0   \n",
       "4       0.165381  7.686258e-01  3.182042e-02  3.212402e-02  2.048445e-03    0   \n",
       "...          ...           ...           ...           ...           ...  ...   \n",
       "180025  0.000000  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00    4   \n",
       "180026  0.919621  6.639876e-02  1.197097e-03  1.278253e-02  5.427218e-07    0   \n",
       "180027  0.143455  7.962033e-01  3.180051e-02  2.622961e-02  2.312066e-03    0   \n",
       "180028  1.000000  3.190462e-40  6.458865e-41  5.076800e-08  2.220121e-30    0   \n",
       "180029  0.999996  1.855692e-07  4.661761e-09  3.851798e-06  2.728606e-09    0   \n",
       "\n",
       "          tvt  \n",
       "0       train  \n",
       "1       train  \n",
       "2       train  \n",
       "3       train  \n",
       "4       train  \n",
       "...       ...  \n",
       "180025   test  \n",
       "180026   test  \n",
       "180027   test  \n",
       "180028   test  \n",
       "180029   test  \n",
       "\n",
       "[600098 rows x 7 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot_multi_prob_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9ec8660",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71514598",
   "metadata": {},
   "source": [
    "# ToN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e46a6263",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_name = 'NF-ToN-IoT'\n",
    "n_epochs = 5000\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6da1649",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/miniconda2/envs/iot/lib/python3.9/site-packages/category_encoders/target_encoder.py:92: FutureWarning: Default parameter min_samples_leaf will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter min_samples_leaf will change in version 2.6.\"\n",
      "/home/hoang/miniconda2/envs/iot/lib/python3.9/site-packages/category_encoders/target_encoder.py:97: FutureWarning: Default parameter smoothing will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter smoothing will change in version 2.6.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PROTOCOL', 'L7_PROTO', 'IN_BYTES', 'OUT_BYTES', 'OUT_PKTS', 'IN_PKTS', 'FLOW_DURATION_MILLISECONDS', 'TCP_FLAGS']\n",
      "['PROTOCOL', 'L7_PROTO', 'IN_BYTES', 'OUT_BYTES', 'OUT_PKTS', 'IN_PKTS', 'FLOW_DURATION_MILLISECONDS', 'TCP_FLAGS']\n",
      "0010 - Training acc: 0.5341001749038696\n",
      "0020 - Training acc: 0.45612844824790955\n",
      "0030 - Training acc: 0.7232189178466797\n",
      "0040 - Training acc: 0.5973123908042908\n",
      "0050 - Training acc: 0.8347201943397522\n",
      "0060 - Training acc: 0.428486168384552\n",
      "0070 - Training acc: 0.7767063975334167\n",
      "0080 - Training acc: 0.5672537088394165\n",
      "0090 - Training acc: 0.5369464159011841\n",
      "0100 - Training acc: 0.6157498955726624\n",
      "0110 - Training acc: 0.5816155076026917\n",
      "0120 - Training acc: 0.37375369668006897\n",
      "0130 - Training acc: 0.5792653560638428\n",
      "0140 - Training acc: 0.4614553451538086\n",
      "0150 - Training acc: 0.8800738453865051\n",
      "0160 - Training acc: 0.899925172328949\n",
      "0170 - Training acc: 0.8864323496818542\n",
      "0180 - Training acc: 0.906700074672699\n",
      "0190 - Training acc: 0.9143035411834717\n",
      "0200 - Training acc: 0.9155392050743103\n",
      "0210 - Training acc: 0.9345567226409912\n",
      "0220 - Training acc: 0.9030770063400269\n",
      "0230 - Training acc: 0.902030885219574\n",
      "0240 - Training acc: 0.9540683031082153\n",
      "0250 - Training acc: 0.9644973278045654\n",
      "0260 - Training acc: 0.924446702003479\n",
      "0270 - Training acc: 0.9125924706459045\n",
      "0280 - Training acc: 0.9349637627601624\n",
      "0290 - Training acc: 0.9278295040130615\n",
      "0300 - Training acc: 0.9187831878662109\n",
      "0310 - Training acc: 0.9352082014083862\n",
      "0320 - Training acc: 0.9760180711746216\n",
      "0330 - Training acc: 0.9687511920928955\n",
      "0340 - Training acc: 0.9769574999809265\n",
      "0350 - Training acc: 0.9211623072624207\n",
      "0360 - Training acc: 0.9747793078422546\n",
      "0370 - Training acc: 0.9814857840538025\n",
      "0380 - Training acc: 0.9548140168190002\n",
      "0390 - Training acc: 0.9805898666381836\n",
      "0400 - Training acc: 0.9628193974494934\n",
      "0410 - Training acc: 0.9816184043884277\n",
      "0420 - Training acc: 0.9830207824707031\n",
      "0430 - Training acc: 0.9709179997444153\n",
      "0440 - Training acc: 0.8632656335830688\n",
      "0450 - Training acc: 0.7419692277908325\n",
      "0460 - Training acc: 0.9825008511543274\n",
      "0470 - Training acc: 0.9797053337097168\n",
      "0480 - Training acc: 0.9623398184776306\n",
      "0490 - Training acc: 0.980627179145813\n",
      "0500 - Training acc: 0.9820098876953125\n",
      "0510 - Training acc: 0.97532719373703\n",
      "0520 - Training acc: 0.9817116260528564\n",
      "0530 - Training acc: 0.9826686382293701\n",
      "0540 - Training acc: 0.981864869594574\n",
      "0550 - Training acc: 0.9811543822288513\n",
      "0560 - Training acc: 0.9823237061500549\n",
      "0570 - Training acc: 0.9747347235679626\n",
      "0580 - Training acc: 0.9822916388511658\n",
      "0590 - Training acc: 0.9823993444442749\n",
      "0600 - Training acc: 0.9750548005104065\n",
      "0610 - Training acc: 0.9821838736534119\n",
      "0620 - Training acc: 0.9745131134986877\n",
      "0630 - Training acc: 0.9823247790336609\n",
      "0640 - Training acc: 0.974610447883606\n",
      "0650 - Training acc: 0.9750392436981201\n",
      "0660 - Training acc: 0.9822812676429749\n",
      "0670 - Training acc: 0.9817934036254883\n",
      "0680 - Training acc: 0.9821010231971741\n",
      "0690 - Training acc: 0.9748963117599487\n",
      "0700 - Training acc: 0.9753913879394531\n",
      "0710 - Training acc: 0.9755529761314392\n",
      "0720 - Training acc: 0.9819374084472656\n",
      "0730 - Training acc: 0.9823858737945557\n",
      "0740 - Training acc: 0.9821932315826416\n",
      "0750 - Training acc: 0.9822874665260315\n",
      "0760 - Training acc: 0.982485294342041\n",
      "0770 - Training acc: 0.982401430606842\n",
      "0780 - Training acc: 0.983639121055603\n",
      "0790 - Training acc: 0.9830549955368042\n",
      "0800 - Training acc: 0.9754660129547119\n",
      "0810 - Training acc: 0.982187032699585\n",
      "0820 - Training acc: 0.9753406643867493\n",
      "0830 - Training acc: 0.9822968244552612\n",
      "0840 - Training acc: 0.9819923043251038\n",
      "0850 - Training acc: 0.9759869575500488\n",
      "0860 - Training acc: 0.9838981032371521\n",
      "0870 - Training acc: 0.9749491214752197\n",
      "0880 - Training acc: 0.9751532077789307\n",
      "0890 - Training acc: 0.9744457602500916\n",
      "0900 - Training acc: 0.983542799949646\n",
      "0910 - Training acc: 0.9830425381660461\n",
      "0920 - Training acc: 0.9829462170600891\n",
      "0930 - Training acc: 0.975049614906311\n",
      "0940 - Training acc: 0.9909899234771729\n",
      "0950 - Training acc: 0.9830694794654846\n",
      "0960 - Training acc: 0.9830145835876465\n",
      "0970 - Training acc: 0.9908562898635864\n",
      "0980 - Training acc: 0.9833025336265564\n",
      "0990 - Training acc: 0.9913120269775391\n",
      "1000 - Training acc: 0.9831202030181885\n",
      "1010 - Training acc: 0.9842367768287659\n",
      "1020 - Training acc: 0.991845428943634\n",
      "1030 - Training acc: 0.9832186102867126\n",
      "1040 - Training acc: 0.9837489128112793\n",
      "1050 - Training acc: 0.982485294342041\n",
      "1060 - Training acc: 0.9833273887634277\n",
      "1070 - Training acc: 0.9902317523956299\n",
      "1080 - Training acc: 0.9835324287414551\n",
      "1090 - Training acc: 0.9755508899688721\n",
      "1100 - Training acc: 0.9911970496177673\n",
      "1110 - Training acc: 0.983031153678894\n",
      "1120 - Training acc: 0.9837862253189087\n",
      "1130 - Training acc: 0.9831730723381042\n",
      "1140 - Training acc: 0.9843165278434753\n",
      "1150 - Training acc: 0.9831740856170654\n",
      "1160 - Training acc: 0.9834951758384705\n",
      "1170 - Training acc: 0.9834744334220886\n",
      "1180 - Training acc: 0.984591007232666\n",
      "1190 - Training acc: 0.9832082390785217\n",
      "1200 - Training acc: 0.9909319281578064\n",
      "1210 - Training acc: 0.9830104112625122\n",
      "1220 - Training acc: 0.9908500909805298\n",
      "1230 - Training acc: 0.9910903573036194\n",
      "1240 - Training acc: 0.9834703207015991\n",
      "1250 - Training acc: 0.9762625098228455\n",
      "1260 - Training acc: 0.98433518409729\n",
      "1270 - Training acc: 0.9833812117576599\n",
      "1280 - Training acc: 0.9838607907295227\n",
      "1290 - Training acc: 0.9829120635986328\n",
      "1300 - Training acc: 0.9828395247459412\n",
      "1310 - Training acc: 0.983938455581665\n",
      "1320 - Training acc: 0.9832020401954651\n",
      "1330 - Training acc: 0.9822853803634644\n",
      "1340 - Training acc: 0.9896330833435059\n",
      "1350 - Training acc: 0.9888893961906433\n",
      "1360 - Training acc: 0.9842202067375183\n",
      "1370 - Training acc: 0.9908552765846252\n",
      "1380 - Training acc: 0.9842408895492554\n",
      "1390 - Training acc: 0.9849162101745605\n",
      "1400 - Training acc: 0.227815642952919\n",
      "1410 - Training acc: 0.2880624830722809\n",
      "1420 - Training acc: 0.8907918930053711\n",
      "1430 - Training acc: 0.9364531636238098\n",
      "1440 - Training acc: 0.9700220227241516\n",
      "1450 - Training acc: 0.9380441308021545\n",
      "1460 - Training acc: 0.9563065767288208\n",
      "1470 - Training acc: 0.9556385278701782\n",
      "1480 - Training acc: 0.9592201709747314\n",
      "1490 - Training acc: 0.9622870087623596\n",
      "1500 - Training acc: 0.954277515411377\n",
      "1510 - Training acc: 0.955250084400177\n",
      "1520 - Training acc: 0.9608649015426636\n",
      "1530 - Training acc: 0.9528917074203491\n",
      "1540 - Training acc: 0.9616572856903076\n",
      "1550 - Training acc: 0.9785784482955933\n",
      "1560 - Training acc: 0.9791035652160645\n",
      "1570 - Training acc: 0.9701173305511475\n",
      "1580 - Training acc: 0.9555193781852722\n",
      "1590 - Training acc: 0.9699371457099915\n",
      "1600 - Training acc: 0.9806468486785889\n",
      "1610 - Training acc: 0.9754442572593689\n",
      "1620 - Training acc: 0.9815883636474609\n",
      "1630 - Training acc: 0.9816825985908508\n",
      "1640 - Training acc: 0.9547197818756104\n",
      "1650 - Training acc: 0.979174017906189\n",
      "1660 - Training acc: 0.9815738201141357\n",
      "1670 - Training acc: 0.943273663520813\n",
      "1680 - Training acc: 0.9785349369049072\n",
      "1690 - Training acc: 0.9796048402786255\n",
      "1700 - Training acc: 0.9826530814170837\n",
      "1710 - Training acc: 0.9706289768218994\n",
      "1720 - Training acc: 0.9805867671966553\n",
      "1730 - Training acc: 0.9824708104133606\n",
      "1740 - Training acc: 0.9782687425613403\n",
      "1750 - Training acc: 0.9621865749359131\n",
      "1760 - Training acc: 0.9894445538520813\n",
      "1770 - Training acc: 0.9892094135284424\n",
      "1780 - Training acc: 0.989771842956543\n",
      "1790 - Training acc: 0.9828612804412842\n",
      "1800 - Training acc: 0.9891700744628906\n",
      "1810 - Training acc: 0.9894010424613953\n",
      "1820 - Training acc: 0.9832911491394043\n",
      "1830 - Training acc: 0.9893979430198669\n",
      "1840 - Training acc: 0.9909515976905823\n",
      "1850 - Training acc: 0.9820782542228699\n",
      "1860 - Training acc: 0.9819850325584412\n",
      "1870 - Training acc: 0.9822770953178406\n",
      "1880 - Training acc: 0.9813522100448608\n",
      "1890 - Training acc: 0.9826634526252747\n",
      "1900 - Training acc: 0.9815862774848938\n",
      "1910 - Training acc: 0.9891721606254578\n",
      "1920 - Training acc: 0.9835034608840942\n",
      "1930 - Training acc: 0.9864905476570129\n",
      "1940 - Training acc: 0.9878619313240051\n",
      "1950 - Training acc: 0.866012454032898\n",
      "1960 - Training acc: 0.8964833617210388\n",
      "1970 - Training acc: 0.9748383164405823\n",
      "1980 - Training acc: 0.9753934741020203\n",
      "1990 - Training acc: 0.9810414910316467\n",
      "2000 - Training acc: 0.974075973033905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010 - Training acc: 0.9697931408882141\n",
      "2020 - Training acc: 0.9828830361366272\n",
      "2030 - Training acc: 0.986673891544342\n",
      "2040 - Training acc: 0.9875863790512085\n",
      "2050 - Training acc: 0.984291672706604\n",
      "2060 - Training acc: 0.9713716506958008\n",
      "2070 - Training acc: 0.9821310639381409\n",
      "2080 - Training acc: 0.9891576766967773\n",
      "2090 - Training acc: 0.9713291525840759\n",
      "2100 - Training acc: 0.9833709001541138\n",
      "2110 - Training acc: 0.9879520535469055\n",
      "2120 - Training acc: 0.9830560088157654\n",
      "2130 - Training acc: 0.9829503893852234\n",
      "2140 - Training acc: 0.9849421381950378\n",
      "2150 - Training acc: 0.9881581664085388\n",
      "2160 - Training acc: 0.9889691472053528\n",
      "2170 - Training acc: 0.9830933213233948\n",
      "2180 - Training acc: 0.9803371429443359\n",
      "2190 - Training acc: 0.9813739657402039\n",
      "2200 - Training acc: 0.9862989783287048\n",
      "2210 - Training acc: 0.9829089045524597\n",
      "2220 - Training acc: 0.9833750128746033\n",
      "2230 - Training acc: 0.9824532270431519\n",
      "2240 - Training acc: 0.9838918447494507\n",
      "2250 - Training acc: 0.9840078949928284\n",
      "2260 - Training acc: 0.988502025604248\n",
      "2270 - Training acc: 0.9831823706626892\n",
      "2280 - Training acc: 0.9862481951713562\n",
      "2290 - Training acc: 0.989612340927124\n",
      "2300 - Training acc: 0.9822677969932556\n",
      "2310 - Training acc: 0.9829213619232178\n",
      "2320 - Training acc: 0.984001636505127\n",
      "2330 - Training acc: 0.9866676926612854\n",
      "2340 - Training acc: 0.9833149313926697\n",
      "2350 - Training acc: 0.9827266335487366\n",
      "2360 - Training acc: 0.9836888313293457\n",
      "2370 - Training acc: 0.9898484945297241\n",
      "2380 - Training acc: 0.9818596839904785\n",
      "2390 - Training acc: 0.9600352644920349\n",
      "2400 - Training acc: 0.9888997673988342\n",
      "2410 - Training acc: 0.9905559420585632\n",
      "2420 - Training acc: 0.9839415550231934\n",
      "2430 - Training acc: 0.9810456037521362\n",
      "2440 - Training acc: 0.982089638710022\n",
      "2450 - Training acc: 0.9824169278144836\n",
      "2460 - Training acc: 0.9834278225898743\n",
      "2470 - Training acc: 0.9876319766044617\n",
      "2480 - Training acc: 0.9831295609474182\n",
      "2490 - Training acc: 0.9902990460395813\n",
      "2500 - Training acc: 0.9809130430221558\n",
      "2510 - Training acc: 0.9889183640480042\n",
      "2520 - Training acc: 0.9817001819610596\n",
      "2530 - Training acc: 0.9870623350143433\n",
      "2540 - Training acc: 0.9820420145988464\n",
      "2550 - Training acc: 0.9835479855537415\n",
      "2560 - Training acc: 0.9820140600204468\n",
      "2570 - Training acc: 0.9842906594276428\n",
      "2580 - Training acc: 0.9892094135284424\n",
      "2590 - Training acc: 0.9834402799606323\n",
      "2600 - Training acc: 0.989247739315033\n",
      "2610 - Training acc: 0.9835304021835327\n",
      "2620 - Training acc: 0.9830756783485413\n",
      "2630 - Training acc: 0.9896413683891296\n",
      "2640 - Training acc: 0.9838877320289612\n",
      "2650 - Training acc: 0.9858421683311462\n",
      "2660 - Training acc: 0.9835169315338135\n",
      "2670 - Training acc: 0.9833097457885742\n",
      "2680 - Training acc: 0.9834495782852173\n",
      "2690 - Training acc: 0.9835096597671509\n",
      "2700 - Training acc: 0.983437180519104\n",
      "2710 - Training acc: 0.9822874665260315\n",
      "2720 - Training acc: 0.9893326759338379\n",
      "2730 - Training acc: 0.9874528050422668\n",
      "2740 - Training acc: 0.982869565486908\n",
      "2750 - Training acc: 0.9805888533592224\n",
      "2760 - Training acc: 0.9831450581550598\n",
      "2770 - Training acc: 0.9882389307022095\n",
      "2780 - Training acc: 0.9885714054107666\n",
      "2790 - Training acc: 0.9820772409439087\n",
      "2800 - Training acc: 0.9889795184135437\n",
      "2810 - Training acc: 0.9904492497444153\n",
      "2820 - Training acc: 0.9816370010375977\n",
      "2830 - Training acc: 0.9899592995643616\n",
      "2840 - Training acc: 0.9826478958129883\n",
      "2850 - Training acc: 0.9883497357368469\n",
      "2860 - Training acc: 0.9820917248725891\n",
      "2870 - Training acc: 0.9839840531349182\n",
      "2880 - Training acc: 0.9901271462440491\n",
      "2890 - Training acc: 0.9817675352096558\n",
      "2900 - Training acc: 0.9837116599082947\n",
      "2910 - Training acc: 0.981182336807251\n",
      "2920 - Training acc: 0.982367217540741\n",
      "2930 - Training acc: 0.9832093119621277\n",
      "2940 - Training acc: 0.981587290763855\n",
      "2950 - Training acc: 0.9831057190895081\n",
      "2960 - Training acc: 0.983555257320404\n",
      "2970 - Training acc: 0.982763946056366\n",
      "2980 - Training acc: 0.9839985370635986\n",
      "2990 - Training acc: 0.9861415028572083\n",
      "3000 - Training acc: 0.981480598449707\n",
      "3010 - Training acc: 0.9818949103355408\n",
      "3020 - Training acc: 0.9871120452880859\n",
      "3030 - Training acc: 0.9676263332366943\n",
      "3040 - Training acc: 0.9889805316925049\n",
      "3050 - Training acc: 0.9903901815414429\n",
      "3060 - Training acc: 0.9911121129989624\n",
      "3070 - Training acc: 0.9821973443031311\n",
      "3080 - Training acc: 0.9821393489837646\n",
      "3090 - Training acc: 0.9837779402732849\n",
      "3100 - Training acc: 0.9829462170600891\n",
      "3110 - Training acc: 0.9875138998031616\n",
      "3120 - Training acc: 0.9873751401901245\n",
      "3130 - Training acc: 0.9890996217727661\n",
      "3140 - Training acc: 0.9823941588401794\n",
      "3150 - Training acc: 0.9885041117668152\n",
      "3160 - Training acc: 0.9826738238334656\n",
      "3170 - Training acc: 0.9843766093254089\n",
      "3180 - Training acc: 0.9823837876319885\n",
      "3190 - Training acc: 0.9840907454490662\n",
      "3200 - Training acc: 0.9913886785507202\n",
      "3210 - Training acc: 0.9825008511543274\n",
      "3220 - Training acc: 0.9884336590766907\n",
      "3230 - Training acc: 0.9910323619842529\n",
      "3240 - Training acc: 0.9833957552909851\n",
      "3250 - Training acc: 0.988714337348938\n",
      "3260 - Training acc: 0.9862917065620422\n",
      "3270 - Training acc: 0.984313428401947\n",
      "3280 - Training acc: 0.9886501431465149\n",
      "3290 - Training acc: 0.9819145798683167\n",
      "3300 - Training acc: 0.9817126393318176\n",
      "3310 - Training acc: 0.9915647506713867\n",
      "3320 - Training acc: 0.9841901659965515\n",
      "3330 - Training acc: 0.9878525733947754\n",
      "3340 - Training acc: 0.9838545918464661\n",
      "3350 - Training acc: 0.9857645034790039\n",
      "3360 - Training acc: 0.9521863460540771\n",
      "3370 - Training acc: 0.9895605444908142\n",
      "3380 - Training acc: 0.9846189618110657\n",
      "3390 - Training acc: 0.9889225363731384\n",
      "3400 - Training acc: 0.9819601774215698\n",
      "3410 - Training acc: 0.9837758541107178\n",
      "3420 - Training acc: 0.990670919418335\n",
      "3430 - Training acc: 0.982700765132904\n",
      "3440 - Training acc: 0.9830984473228455\n",
      "3450 - Training acc: 0.9825515747070312\n",
      "3460 - Training acc: 0.9822957515716553\n",
      "3470 - Training acc: 0.9875532388687134\n",
      "3480 - Training acc: 0.9836038947105408\n",
      "3490 - Training acc: 0.9823817610740662\n",
      "3500 - Training acc: 0.9895418882369995\n",
      "3510 - Training acc: 0.9841217994689941\n",
      "3520 - Training acc: 0.9825754165649414\n",
      "3530 - Training acc: 0.985454797744751\n",
      "3540 - Training acc: 0.9833160042762756\n",
      "3550 - Training acc: 0.9842077493667603\n",
      "3560 - Training acc: 0.9907341003417969\n",
      "3570 - Training acc: 0.9842813014984131\n",
      "3580 - Training acc: 0.9821538925170898\n",
      "3590 - Training acc: 0.9846158623695374\n",
      "3600 - Training acc: 0.9840410351753235\n",
      "3610 - Training acc: 0.9821217656135559\n",
      "3620 - Training acc: 0.9891068935394287\n",
      "3630 - Training acc: 0.984634518623352\n",
      "3640 - Training acc: 0.9889007806777954\n",
      "3650 - Training acc: 0.9839218854904175\n",
      "3660 - Training acc: 0.9820264577865601\n",
      "3670 - Training acc: 0.9818224310874939\n",
      "3680 - Training acc: 0.9822377562522888\n",
      "3690 - Training acc: 0.9862461090087891\n",
      "3700 - Training acc: 0.9889100790023804\n",
      "3710 - Training acc: 0.9719724059104919\n",
      "3720 - Training acc: 0.9874434471130371\n",
      "3730 - Training acc: 0.9843123555183411\n",
      "3740 - Training acc: 0.987850546836853\n",
      "3750 - Training acc: 0.9846168756484985\n",
      "3760 - Training acc: 0.9841611385345459\n",
      "3770 - Training acc: 0.9829431176185608\n",
      "3780 - Training acc: 0.9830394387245178\n",
      "3790 - Training acc: 0.9914218187332153\n",
      "3800 - Training acc: 0.9830083847045898\n",
      "3810 - Training acc: 0.9845961928367615\n",
      "3820 - Training acc: 0.9881964921951294\n",
      "3830 - Training acc: 0.9846904277801514\n",
      "3840 - Training acc: 0.9860193133354187\n",
      "3850 - Training acc: 0.9823962450027466\n",
      "3860 - Training acc: 0.9889857172966003\n",
      "3870 - Training acc: 0.830913782119751\n",
      "3880 - Training acc: 0.33872339129447937\n",
      "3890 - Training acc: 0.9713436961174011\n",
      "3900 - Training acc: 0.9658002853393555\n",
      "3910 - Training acc: 0.9707667827606201\n",
      "3920 - Training acc: 0.9913979768753052\n",
      "3930 - Training acc: 0.9820595979690552\n",
      "3940 - Training acc: 0.9822118878364563\n",
      "3950 - Training acc: 0.9896227121353149\n",
      "3960 - Training acc: 0.9871845245361328\n",
      "3970 - Training acc: 0.9876951575279236\n",
      "3980 - Training acc: 0.9897211194038391\n",
      "3990 - Training acc: 0.9685150384902954\n",
      "4000 - Training acc: 0.9840855598449707\n",
      "4010 - Training acc: 0.9842440485954285\n",
      "4020 - Training acc: 0.9911121129989624\n",
      "4030 - Training acc: 0.9833232164382935\n",
      "4040 - Training acc: 0.9823693037033081\n",
      "4050 - Training acc: 0.9606267213821411\n",
      "4060 - Training acc: 0.9890478849411011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4070 - Training acc: 0.9913855791091919\n",
      "4080 - Training acc: 0.9834475517272949\n",
      "4090 - Training acc: 0.9835935831069946\n",
      "4100 - Training acc: 0.9895553588867188\n",
      "4110 - Training acc: 0.9904430508613586\n",
      "4120 - Training acc: 0.9838794469833374\n",
      "4130 - Training acc: 0.971332311630249\n",
      "4140 - Training acc: 0.9822242856025696\n",
      "4150 - Training acc: 0.9879748225212097\n",
      "4160 - Training acc: 0.9896144270896912\n",
      "4170 - Training acc: 0.9833439588546753\n",
      "4180 - Training acc: 0.991696298122406\n",
      "4190 - Training acc: 0.9918485283851624\n",
      "4200 - Training acc: 0.9831771850585938\n",
      "4210 - Training acc: 0.9847214818000793\n",
      "4220 - Training acc: 0.9816712141036987\n",
      "4230 - Training acc: 0.9866220951080322\n",
      "4240 - Training acc: 0.9887112379074097\n",
      "4250 - Training acc: 0.990348756313324\n",
      "4260 - Training acc: 0.9830808639526367\n",
      "4270 - Training acc: 0.9898298382759094\n",
      "4280 - Training acc: 0.9913855791091919\n",
      "4290 - Training acc: 0.9919241666793823\n",
      "4300 - Training acc: 0.9816049337387085\n",
      "4310 - Training acc: 0.9836556911468506\n",
      "4320 - Training acc: 0.9888769388198853\n",
      "4330 - Training acc: 0.9842957854270935\n",
      "4340 - Training acc: 0.9922680258750916\n",
      "4350 - Training acc: 0.9844014644622803\n",
      "4360 - Training acc: 0.9839653968811035\n",
      "4370 - Training acc: 0.9821134805679321\n",
      "4380 - Training acc: 0.9836857318878174\n",
      "4390 - Training acc: 0.984397292137146\n",
      "4400 - Training acc: 0.9828840494155884\n",
      "4410 - Training acc: 0.9914145469665527\n",
      "4420 - Training acc: 0.9918091893196106\n",
      "4430 - Training acc: 0.9844874143600464\n",
      "4440 - Training acc: 0.9849131107330322\n",
      "4450 - Training acc: 0.9898629784584045\n",
      "4460 - Training acc: 0.965156078338623\n",
      "4470 - Training acc: 0.982441782951355\n",
      "4480 - Training acc: 0.9903218150138855\n",
      "4490 - Training acc: 0.9907423853874207\n",
      "4500 - Training acc: 0.9848520159721375\n",
      "4510 - Training acc: 0.9841901659965515\n",
      "4520 - Training acc: 0.9848281741142273\n",
      "4530 - Training acc: 0.9848229885101318\n",
      "4540 - Training acc: 0.9848934412002563\n",
      "4550 - Training acc: 0.9845371246337891\n",
      "4560 - Training acc: 0.9842357635498047\n",
      "4570 - Training acc: 0.9837685823440552\n",
      "4580 - Training acc: 0.9842668175697327\n",
      "4590 - Training acc: 0.991705596446991\n",
      "4600 - Training acc: 0.9838690757751465\n",
      "4610 - Training acc: 0.9848955273628235\n",
      "4620 - Training acc: 0.984643816947937\n",
      "4630 - Training acc: 0.9823278784751892\n",
      "4640 - Training acc: 0.9836629629135132\n",
      "4650 - Training acc: 0.992604672908783\n",
      "4660 - Training acc: 0.9843103289604187\n",
      "4670 - Training acc: 0.9839094877243042\n",
      "4680 - Training acc: 0.984322726726532\n",
      "4690 - Training acc: 0.984578549861908\n",
      "4700 - Training acc: 0.9850136041641235\n",
      "4710 - Training acc: 0.9912623167037964\n",
      "4720 - Training acc: 0.9912840723991394\n",
      "4730 - Training acc: 0.9825205206871033\n",
      "4740 - Training acc: 0.9844781160354614\n",
      "4750 - Training acc: 0.9827908277511597\n",
      "4760 - Training acc: 0.9921106100082397\n",
      "4770 - Training acc: 0.9923394918441772\n",
      "4780 - Training acc: 0.9926429986953735\n",
      "4790 - Training acc: 0.9853522777557373\n",
      "4800 - Training acc: 0.984696626663208\n",
      "4810 - Training acc: 0.9845184683799744\n",
      "4820 - Training acc: 0.984462559223175\n",
      "4830 - Training acc: 0.9842657446861267\n",
      "4840 - Training acc: 0.99196457862854\n",
      "4850 - Training acc: 0.9818203449249268\n",
      "4860 - Training acc: 0.9846821427345276\n",
      "4870 - Training acc: 0.9846552014350891\n",
      "4880 - Training acc: 0.9843103289604187\n",
      "4890 - Training acc: 0.9851689338684082\n",
      "4900 - Training acc: 0.9723100662231445\n",
      "4910 - Training acc: 0.9921923875808716\n",
      "4920 - Training acc: 0.9848944544792175\n",
      "4930 - Training acc: 0.9846686720848083\n",
      "4940 - Training acc: 0.9845848083496094\n",
      "4950 - Training acc: 0.9845050573348999\n",
      "4960 - Training acc: 0.9840772747993469\n",
      "4970 - Training acc: 0.9843589663505554\n",
      "4980 - Training acc: 0.9843113422393799\n",
      "4990 - Training acc: 0.9841725826263428\n",
      "5000 - Training acc: 0.9852290153503418\n",
      "<class 'torch.Tensor'>\n",
      "965478\n",
      "413782\n"
     ]
    }
   ],
   "source": [
    "cname_label = 'Label'\n",
    "ton_bin_prob_df = run_baseline(\n",
    "    ds_name,\n",
    "    cname_label,\n",
    "    n_epochs\n",
    ")\n",
    "ton_bin_prob_df.to_csv('../output/EGraphSAGE_nf_ton_binary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2aa7d91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65a10f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/miniconda2/envs/iot/lib/python3.9/site-packages/category_encoders/target_encoder.py:92: FutureWarning: Default parameter min_samples_leaf will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter min_samples_leaf will change in version 2.6.\"\n",
      "/home/hoang/miniconda2/envs/iot/lib/python3.9/site-packages/category_encoders/target_encoder.py:97: FutureWarning: Default parameter smoothing will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter smoothing will change in version 2.6.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OUT_BYTES', 'FLOW_DURATION_MILLISECONDS', 'IN_BYTES', 'OUT_PKTS', 'IN_PKTS', 'PROTOCOL', 'TCP_FLAGS', 'L7_PROTO']\n",
      "['OUT_BYTES', 'FLOW_DURATION_MILLISECONDS', 'IN_BYTES', 'OUT_PKTS', 'IN_PKTS', 'PROTOCOL', 'TCP_FLAGS', 'L7_PROTO']\n",
      "0010 - Training acc: 0.07244893908500671\n",
      "0020 - Training acc: 0.07244893908500671\n",
      "0030 - Training acc: 0.07244893908500671\n",
      "0040 - Training acc: 0.07254216074943542\n",
      "0050 - Training acc: 0.07254216074943542\n",
      "0060 - Training acc: 0.07254733890295029\n",
      "0070 - Training acc: 0.07256080210208893\n",
      "0080 - Training acc: 0.07258358597755432\n",
      "0090 - Training acc: 0.07260948419570923\n",
      "0100 - Training acc: 0.07263537496328354\n",
      "0110 - Training acc: 0.07267266511917114\n",
      "0120 - Training acc: 0.07271409034729004\n",
      "0130 - Training acc: 0.07276173681020737\n",
      "0140 - Training acc: 0.0728052407503128\n",
      "0150 - Training acc: 0.07647388428449631\n",
      "0160 - Training acc: 0.07289949059486389\n",
      "0170 - Training acc: 0.080472931265831\n",
      "0180 - Training acc: 0.08319281786680222\n",
      "0190 - Training acc: 0.08330157399177551\n",
      "0200 - Training acc: 0.08364647626876831\n",
      "0210 - Training acc: 0.0865061953663826\n",
      "0220 - Training acc: 0.08382462710142136\n",
      "0230 - Training acc: 0.08693085610866547\n",
      "0240 - Training acc: 0.08426482230424881\n",
      "0250 - Training acc: 0.0872291550040245\n",
      "0260 - Training acc: 0.0873907282948494\n",
      "0270 - Training acc: 0.08769109845161438\n",
      "0280 - Training acc: 0.08814476430416107\n",
      "0290 - Training acc: 0.08777085691690445\n",
      "0300 - Training acc: 0.08811679482460022\n",
      "0310 - Training acc: 0.08756370097398758\n",
      "0320 - Training acc: 0.08812197297811508\n",
      "0330 - Training acc: 0.08760720491409302\n",
      "0340 - Training acc: 0.09433753788471222\n",
      "0350 - Training acc: 0.09477151930332184\n",
      "0360 - Training acc: 0.0946306511759758\n",
      "0370 - Training acc: 0.09486162662506104\n",
      "0380 - Training acc: 0.09753283858299255\n",
      "0390 - Training acc: 0.09975452721118927\n",
      "0400 - Training acc: 0.09977835416793823\n",
      "0410 - Training acc: 0.09266272187232971\n",
      "0420 - Training acc: 0.15616481006145477\n",
      "0430 - Training acc: 0.09230952709913254\n",
      "0440 - Training acc: 0.12495028972625732\n",
      "0450 - Training acc: 0.12999647855758667\n",
      "0460 - Training acc: 0.14592120051383972\n",
      "0470 - Training acc: 0.13008660078048706\n",
      "0480 - Training acc: 0.16389153897762299\n",
      "0490 - Training acc: 0.16845300793647766\n",
      "0500 - Training acc: 0.16341716051101685\n",
      "0510 - Training acc: 0.16936032474040985\n",
      "0520 - Training acc: 0.16899573802947998\n",
      "0530 - Training acc: 0.16856175661087036\n",
      "0540 - Training acc: 0.1669190526008606\n",
      "0550 - Training acc: 0.1660345196723938\n",
      "0560 - Training acc: 0.2034863531589508\n",
      "0570 - Training acc: 0.1644301414489746\n",
      "0580 - Training acc: 0.16925571858882904\n",
      "0590 - Training acc: 0.16647885739803314\n",
      "0600 - Training acc: 0.16091684997081757\n",
      "0610 - Training acc: 0.16959233582019806\n",
      "0620 - Training acc: 0.16952086985111237\n",
      "0630 - Training acc: 0.16412459313869476\n",
      "0640 - Training acc: 0.1690216362476349\n",
      "0650 - Training acc: 0.16985541582107544\n",
      "0660 - Training acc: 0.16638046503067017\n",
      "0670 - Training acc: 0.20378154516220093\n",
      "0680 - Training acc: 0.16629138588905334\n",
      "0690 - Training acc: 0.17142976820468903\n",
      "0700 - Training acc: 0.20982207357883453\n",
      "0710 - Training acc: 0.19938476383686066\n",
      "0720 - Training acc: 0.20037701725959778\n",
      "0730 - Training acc: 0.17342980206012726\n",
      "0740 - Training acc: 0.17338009178638458\n",
      "0750 - Training acc: 0.17168352007865906\n",
      "0760 - Training acc: 0.1695799082517624\n",
      "0770 - Training acc: 0.1758006513118744\n",
      "0780 - Training acc: 0.17567843198776245\n",
      "0790 - Training acc: 0.2057950496673584\n",
      "0800 - Training acc: 0.19262750446796417\n",
      "0810 - Training acc: 0.20921407639980316\n",
      "0820 - Training acc: 0.22409993410110474\n",
      "0830 - Training acc: 0.1829463094472885\n",
      "0840 - Training acc: 0.2220325767993927\n",
      "0850 - Training acc: 0.18504475057125092\n",
      "0860 - Training acc: 0.18510586023330688\n",
      "0870 - Training acc: 0.1948108822107315\n",
      "0880 - Training acc: 0.18010938167572021\n",
      "0890 - Training acc: 0.22760803997516632\n",
      "0900 - Training acc: 0.23003998398780823\n",
      "0910 - Training acc: 0.21047769486904144\n",
      "0920 - Training acc: 0.22482289373874664\n",
      "0930 - Training acc: 0.21161909401416779\n",
      "0940 - Training acc: 0.25479450821876526\n",
      "0950 - Training acc: 0.2339872568845749\n",
      "0960 - Training acc: 0.2299850881099701\n",
      "0970 - Training acc: 0.2613238990306854\n",
      "0980 - Training acc: 0.2546546757221222\n",
      "0990 - Training acc: 0.2386792153120041\n",
      "1000 - Training acc: 0.27521753311157227\n",
      "1010 - Training acc: 0.2608516216278076\n",
      "1020 - Training acc: 0.27996230125427246\n",
      "1030 - Training acc: 0.24964267015457153\n",
      "1040 - Training acc: 0.22833721339702606\n",
      "1050 - Training acc: 0.2825848460197449\n",
      "1060 - Training acc: 0.24131417274475098\n",
      "1070 - Training acc: 0.26994967460632324\n",
      "1080 - Training acc: 0.24966132640838623\n",
      "1090 - Training acc: 0.26510855555534363\n",
      "1100 - Training acc: 0.22663961350917816\n",
      "1110 - Training acc: 0.23471538722515106\n",
      "1120 - Training acc: 0.25661641359329224\n",
      "1130 - Training acc: 0.24307909607887268\n",
      "1140 - Training acc: 0.2686612010002136\n",
      "1150 - Training acc: 0.23620066046714783\n",
      "1160 - Training acc: 0.2926710247993469\n",
      "1170 - Training acc: 0.29707401990890503\n",
      "1180 - Training acc: 0.2883208394050598\n",
      "1190 - Training acc: 0.2914985418319702\n",
      "1200 - Training acc: 0.28743940591812134\n",
      "1210 - Training acc: 0.2508659064769745\n",
      "1220 - Training acc: 0.30454903841018677\n",
      "1230 - Training acc: 0.3018053472042084\n",
      "1240 - Training acc: 0.26096242666244507\n",
      "1250 - Training acc: 0.2955276370048523\n",
      "1260 - Training acc: 0.29898807406425476\n",
      "1270 - Training acc: 0.30173179507255554\n",
      "1280 - Training acc: 0.30061835050582886\n",
      "1290 - Training acc: 0.2609230875968933\n",
      "1300 - Training acc: 0.29866698384284973\n",
      "1310 - Training acc: 0.29917657375335693\n",
      "1320 - Training acc: 0.26032543182373047\n",
      "1330 - Training acc: 0.31167399883270264\n",
      "1340 - Training acc: 0.30894583463668823\n",
      "1350 - Training acc: 0.2988513708114624\n",
      "1360 - Training acc: 0.2996126413345337\n",
      "1370 - Training acc: 0.3135828971862793\n",
      "1380 - Training acc: 0.29630857706069946\n",
      "1390 - Training acc: 0.27179229259490967\n",
      "1400 - Training acc: 0.26962339878082275\n",
      "1410 - Training acc: 0.3071177303791046\n",
      "1420 - Training acc: 0.3108164072036743\n",
      "1430 - Training acc: 0.2705586850643158\n",
      "1440 - Training acc: 0.275459885597229\n",
      "1450 - Training acc: 0.2781466245651245\n",
      "1460 - Training acc: 0.27581825852394104\n",
      "1470 - Training acc: 0.3012905716896057\n",
      "1480 - Training acc: 0.306159645318985\n",
      "1490 - Training acc: 0.31236693263053894\n",
      "1500 - Training acc: 0.3060467541217804\n",
      "1510 - Training acc: 0.31279778480529785\n",
      "1520 - Training acc: 0.2695726454257965\n",
      "1530 - Training acc: 0.27551063895225525\n",
      "1540 - Training acc: 0.3074035942554474\n",
      "1550 - Training acc: 0.3049705922603607\n",
      "1560 - Training acc: 0.3125606179237366\n",
      "1570 - Training acc: 0.3155311346054077\n",
      "1580 - Training acc: 0.3178284466266632\n",
      "1590 - Training acc: 0.31705164909362793\n",
      "1600 - Training acc: 0.3116304874420166\n",
      "1610 - Training acc: 0.28215500712394714\n",
      "1620 - Training acc: 0.3112099766731262\n",
      "1630 - Training acc: 0.2829069495201111\n",
      "1640 - Training acc: 0.31610599160194397\n",
      "1650 - Training acc: 0.3182510435581207\n",
      "1660 - Training acc: 0.28004103899002075\n",
      "1670 - Training acc: 0.27597570419311523\n",
      "1680 - Training acc: 0.3166818618774414\n",
      "1690 - Training acc: 0.311196506023407\n",
      "1700 - Training acc: 0.3088836669921875\n",
      "1710 - Training acc: 0.2782139480113983\n",
      "1720 - Training acc: 0.28176555037498474\n",
      "1730 - Training acc: 0.27756351232528687\n",
      "1740 - Training acc: 0.31792891025543213\n",
      "1750 - Training acc: 0.3193530738353729\n",
      "1760 - Training acc: 0.2795966863632202\n",
      "1770 - Training acc: 0.3187989592552185\n",
      "1780 - Training acc: 0.31441357731819153\n",
      "1790 - Training acc: 0.28173550963401794\n",
      "1800 - Training acc: 0.3193655014038086\n",
      "1810 - Training acc: 0.3215084969997406\n",
      "1820 - Training acc: 0.3162924349308014\n",
      "1830 - Training acc: 0.31919771432876587\n",
      "1840 - Training acc: 0.31581494212150574\n",
      "1850 - Training acc: 0.319551944732666\n",
      "1860 - Training acc: 0.32196006178855896\n",
      "1870 - Training acc: 0.2873762249946594\n",
      "1880 - Training acc: 0.30936530232429504\n",
      "1890 - Training acc: 0.28363820910453796\n",
      "1900 - Training acc: 0.27591872215270996\n",
      "1910 - Training acc: 0.3171044588088989\n",
      "1920 - Training acc: 0.315288782119751\n",
      "1930 - Training acc: 0.32251110672950745\n",
      "1940 - Training acc: 0.3203132152557373\n",
      "1950 - Training acc: 0.31736236810684204\n",
      "1960 - Training acc: 0.3212661147117615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1970 - Training acc: 0.32117703557014465\n",
      "1980 - Training acc: 0.31641775369644165\n",
      "1990 - Training acc: 0.31615155935287476\n",
      "2000 - Training acc: 0.3201226592063904\n",
      "2010 - Training acc: 0.3194297254085541\n",
      "2020 - Training acc: 0.3188673257827759\n",
      "2030 - Training acc: 0.2876621186733246\n",
      "2040 - Training acc: 0.3173644244670868\n",
      "2050 - Training acc: 0.2789483070373535\n",
      "2060 - Training acc: 0.31663838028907776\n",
      "2070 - Training acc: 0.3190433979034424\n",
      "2080 - Training acc: 0.3207451403141022\n",
      "2090 - Training acc: 0.31701332330703735\n",
      "2100 - Training acc: 0.32291609048843384\n",
      "2110 - Training acc: 0.3225908577442169\n",
      "2120 - Training acc: 0.3195115625858307\n",
      "2130 - Training acc: 0.32014337182044983\n",
      "2140 - Training acc: 0.3189760744571686\n",
      "2150 - Training acc: 0.3140324056148529\n",
      "2160 - Training acc: 0.31765133142471313\n",
      "2170 - Training acc: 0.28862327337265015\n",
      "2180 - Training acc: 0.31409040093421936\n",
      "2190 - Training acc: 0.314209520816803\n",
      "2200 - Training acc: 0.3221299350261688\n",
      "2210 - Training acc: 0.3166735768318176\n",
      "2220 - Training acc: 0.32117393612861633\n",
      "2230 - Training acc: 0.31881552934646606\n",
      "2240 - Training acc: 0.32258257269859314\n",
      "2250 - Training acc: 0.28817999362945557\n",
      "2260 - Training acc: 0.3223288059234619\n",
      "2270 - Training acc: 0.32170218229293823\n",
      "2280 - Training acc: 0.3217363655567169\n",
      "2290 - Training acc: 0.32412686944007874\n",
      "2300 - Training acc: 0.32284459471702576\n",
      "2310 - Training acc: 0.3253656327724457\n",
      "2320 - Training acc: 0.3230724632740021\n",
      "2330 - Training acc: 0.31666842103004456\n",
      "2340 - Training acc: 0.279034286737442\n",
      "2350 - Training acc: 0.3234339654445648\n",
      "2360 - Training acc: 0.28608259558677673\n",
      "2370 - Training acc: 0.32449042797088623\n",
      "2380 - Training acc: 0.3182748556137085\n",
      "2390 - Training acc: 0.3236887454986572\n",
      "2400 - Training acc: 0.32112422585487366\n",
      "2410 - Training acc: 0.3236742615699768\n",
      "2420 - Training acc: 0.3154482841491699\n",
      "2430 - Training acc: 0.318195104598999\n",
      "2440 - Training acc: 0.3146797716617584\n",
      "2450 - Training acc: 0.2864223122596741\n",
      "2460 - Training acc: 0.276854008436203\n",
      "2470 - Training acc: 0.32119879126548767\n",
      "2480 - Training acc: 0.28770560026168823\n",
      "2490 - Training acc: 0.32088807225227356\n",
      "2500 - Training acc: 0.31516656279563904\n",
      "2510 - Training acc: 0.322025328874588\n",
      "2520 - Training acc: 0.3259042203426361\n",
      "2530 - Training acc: 0.3198108971118927\n",
      "2540 - Training acc: 0.32296785712242126\n",
      "2550 - Training acc: 0.3219020664691925\n",
      "2560 - Training acc: 0.28314414620399475\n",
      "2570 - Training acc: 0.2869153320789337\n",
      "2580 - Training acc: 0.3195032775402069\n",
      "2590 - Training acc: 0.32192692160606384\n",
      "2600 - Training acc: 0.3207772374153137\n",
      "2610 - Training acc: 0.2899283468723297\n",
      "2620 - Training acc: 0.30484631657600403\n",
      "2630 - Training acc: 0.3246116042137146\n",
      "2640 - Training acc: 0.3149583637714386\n",
      "2650 - Training acc: 0.3182634711265564\n",
      "2660 - Training acc: 0.32063430547714233\n",
      "2670 - Training acc: 0.3238057792186737\n",
      "2680 - Training acc: 0.3202086091041565\n",
      "2690 - Training acc: 0.31535714864730835\n",
      "2700 - Training acc: 0.2897118628025055\n",
      "2710 - Training acc: 0.3233303725719452\n",
      "2720 - Training acc: 0.31627586483955383\n",
      "2730 - Training acc: 0.32508495450019836\n",
      "2740 - Training acc: 0.32348471879959106\n",
      "2750 - Training acc: 0.32731595635414124\n",
      "2760 - Training acc: 0.3156067728996277\n",
      "2770 - Training acc: 0.28944361209869385\n",
      "2780 - Training acc: 0.3262833058834076\n",
      "2790 - Training acc: 0.32373329997062683\n",
      "2800 - Training acc: 0.29093095660209656\n",
      "2810 - Training acc: 0.32377058267593384\n",
      "2820 - Training acc: 0.32273897528648376\n",
      "2830 - Training acc: 0.32540085911750793\n",
      "2840 - Training acc: 0.32310768961906433\n",
      "2850 - Training acc: 0.3247586786746979\n",
      "2860 - Training acc: 0.3228653371334076\n",
      "2870 - Training acc: 0.3239974081516266\n",
      "2880 - Training acc: 0.32438787817955017\n",
      "2890 - Training acc: 0.2886843979358673\n",
      "2900 - Training acc: 0.32690373063087463\n",
      "2910 - Training acc: 0.3279684782028198\n",
      "2920 - Training acc: 0.32414138317108154\n",
      "2930 - Training acc: 0.3263309597969055\n",
      "2940 - Training acc: 0.3256504535675049\n",
      "2950 - Training acc: 0.324461430311203\n",
      "2960 - Training acc: 0.3204675614833832\n",
      "2970 - Training acc: 0.3222987651824951\n",
      "2980 - Training acc: 0.32609790563583374\n",
      "2990 - Training acc: 0.3265536427497864\n",
      "3000 - Training acc: 0.2958124577999115\n",
      "3010 - Training acc: 0.3246333599090576\n",
      "3020 - Training acc: 0.3255002796649933\n",
      "3030 - Training acc: 0.29106664657592773\n",
      "3040 - Training acc: 0.32730767130851746\n",
      "3050 - Training acc: 0.3238948583602905\n",
      "3060 - Training acc: 0.327361524105072\n",
      "3070 - Training acc: 0.3258224129676819\n",
      "3080 - Training acc: 0.32619422674179077\n",
      "3090 - Training acc: 0.32631853222846985\n",
      "3100 - Training acc: 0.31789472699165344\n",
      "3110 - Training acc: 0.3252102732658386\n",
      "3120 - Training acc: 0.3256722092628479\n",
      "3130 - Training acc: 0.3230041265487671\n",
      "3140 - Training acc: 0.32522061467170715\n",
      "3150 - Training acc: 0.32375088334083557\n",
      "3160 - Training acc: 0.3245256245136261\n",
      "3170 - Training acc: 0.32606062293052673\n",
      "3180 - Training acc: 0.3266126811504364\n",
      "3190 - Training acc: 0.3261072337627411\n",
      "3200 - Training acc: 0.32586488127708435\n",
      "3210 - Training acc: 0.31841158866882324\n",
      "3220 - Training acc: 0.3276546597480774\n",
      "3230 - Training acc: 0.32691720128059387\n",
      "3240 - Training acc: 0.3263993263244629\n",
      "3250 - Training acc: 0.32702386379241943\n",
      "3260 - Training acc: 0.3272797167301178\n",
      "3270 - Training acc: 0.328584760427475\n",
      "3280 - Training acc: 0.3283631205558777\n",
      "3290 - Training acc: 0.32335007190704346\n",
      "3300 - Training acc: 0.3246592581272125\n",
      "3310 - Training acc: 0.32710880041122437\n",
      "3320 - Training acc: 0.32714608311653137\n",
      "3330 - Training acc: 0.31930026412010193\n",
      "3340 - Training acc: 0.3277209401130676\n",
      "3350 - Training acc: 0.33107057213783264\n",
      "3360 - Training acc: 0.3220367133617401\n",
      "3370 - Training acc: 0.32718026638031006\n",
      "3380 - Training acc: 0.23701682686805725\n",
      "3390 - Training acc: 0.28953370451927185\n",
      "3400 - Training acc: 0.3010886013507843\n",
      "3410 - Training acc: 0.31779220700263977\n",
      "3420 - Training acc: 0.30347082018852234\n",
      "3430 - Training acc: 0.31481751799583435\n",
      "3440 - Training acc: 0.3165099322795868\n",
      "3450 - Training acc: 0.31505057215690613\n",
      "3460 - Training acc: 0.3071829676628113\n",
      "3470 - Training acc: 0.3191334903240204\n",
      "3480 - Training acc: 0.3253273069858551\n",
      "3490 - Training acc: 0.3191334903240204\n",
      "3500 - Training acc: 0.32043856382369995\n",
      "3510 - Training acc: 0.31143680214881897\n",
      "3520 - Training acc: 0.32308903336524963\n",
      "3530 - Training acc: 0.3158843517303467\n",
      "3540 - Training acc: 0.3193613588809967\n",
      "3550 - Training acc: 0.2802709639072418\n",
      "3560 - Training acc: 0.31987717747688293\n",
      "3570 - Training acc: 0.32009157538414\n",
      "3580 - Training acc: 0.32304346561431885\n",
      "3590 - Training acc: 0.32101234793663025\n",
      "3600 - Training acc: 0.31492212414741516\n",
      "3610 - Training acc: 0.32572296261787415\n",
      "3620 - Training acc: 0.3168289363384247\n",
      "3630 - Training acc: 0.32050174474716187\n",
      "3640 - Training acc: 0.3242853581905365\n",
      "3650 - Training acc: 0.32183992862701416\n",
      "3660 - Training acc: 0.2804698348045349\n",
      "3670 - Training acc: 0.2705172598361969\n",
      "3680 - Training acc: 0.2898848354816437\n",
      "3690 - Training acc: 0.27430295944213867\n",
      "3700 - Training acc: 0.3228321671485901\n",
      "3710 - Training acc: 0.3187906742095947\n",
      "3720 - Training acc: 0.32206469774246216\n",
      "3730 - Training acc: 0.3283216655254364\n",
      "3740 - Training acc: 0.3162965774536133\n",
      "3750 - Training acc: 0.32124125957489014\n",
      "3760 - Training acc: 0.32422319054603577\n",
      "3770 - Training acc: 0.32647284865379333\n",
      "3780 - Training acc: 0.3201899826526642\n",
      "3790 - Training acc: 0.3231128752231598\n",
      "3800 - Training acc: 0.3233293294906616\n",
      "3810 - Training acc: 0.32565149664878845\n",
      "3820 - Training acc: 0.3275189697742462\n",
      "3830 - Training acc: 0.32403677701950073\n",
      "3840 - Training acc: 0.3222055435180664\n",
      "3850 - Training acc: 0.3202345073223114\n",
      "3860 - Training acc: 0.3300151228904724\n",
      "3870 - Training acc: 0.3277095556259155\n",
      "3880 - Training acc: 0.3203090727329254\n",
      "3890 - Training acc: 0.3284718692302704\n",
      "3900 - Training acc: 0.3279664218425751\n",
      "3910 - Training acc: 0.3219403922557831\n",
      "3920 - Training acc: 0.3256991505622864\n",
      "3930 - Training acc: 0.3213334381580353\n",
      "3940 - Training acc: 0.3258399963378906\n",
      "3950 - Training acc: 0.32811763882637024\n",
      "3960 - Training acc: 0.3213841915130615\n",
      "3970 - Training acc: 0.3226965069770813\n",
      "3980 - Training acc: 0.33051541447639465\n",
      "3990 - Training acc: 0.3270072937011719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000 - Training acc: 0.32266438007354736\n",
      "4010 - Training acc: 0.3238834738731384\n",
      "4020 - Training acc: 0.33193543553352356\n",
      "4030 - Training acc: 0.32844701409339905\n",
      "4040 - Training acc: 0.32761943340301514\n",
      "4050 - Training acc: 0.3239104151725769\n",
      "4060 - Training acc: 0.32379958033561707\n",
      "4070 - Training acc: 0.32638999819755554\n",
      "4080 - Training acc: 0.32584312558174133\n",
      "4090 - Training acc: 0.3263092041015625\n",
      "4100 - Training acc: 0.3260502815246582\n",
      "4110 - Training acc: 0.3257405757904053\n",
      "4120 - Training acc: 0.3207409977912903\n",
      "4130 - Training acc: 0.32323405146598816\n",
      "4140 - Training acc: 0.3280844986438751\n",
      "4150 - Training acc: 0.33093902468681335\n",
      "4160 - Training acc: 0.3290487825870514\n",
      "4170 - Training acc: 0.33034244179725647\n",
      "4180 - Training acc: 0.3281010687351227\n",
      "4190 - Training acc: 0.33217158913612366\n",
      "4200 - Training acc: 0.3343176543712616\n",
      "4210 - Training acc: 0.32174670696258545\n",
      "4220 - Training acc: 0.3256608247756958\n",
      "4230 - Training acc: 0.32666343450546265\n",
      "4240 - Training acc: 0.32746925950050354\n",
      "4250 - Training acc: 0.3287007510662079\n",
      "4260 - Training acc: 0.32698971033096313\n",
      "4270 - Training acc: 0.32826367020606995\n",
      "4280 - Training acc: 0.32702285051345825\n",
      "4290 - Training acc: 0.32950761914253235\n",
      "4300 - Training acc: 0.3270694613456726\n",
      "4310 - Training acc: 0.3291647732257843\n",
      "4320 - Training acc: 0.3282605707645416\n",
      "4330 - Training acc: 0.32776960730552673\n",
      "4340 - Training acc: 0.32420456409454346\n",
      "4350 - Training acc: 0.32967644929885864\n",
      "4360 - Training acc: 0.32853400707244873\n",
      "4370 - Training acc: 0.32979971170425415\n",
      "4380 - Training acc: 0.32879915833473206\n",
      "4390 - Training acc: 0.33075985312461853\n",
      "4400 - Training acc: 0.3317427635192871\n",
      "4410 - Training acc: 0.3310229182243347\n",
      "4420 - Training acc: 0.3342234194278717\n",
      "4430 - Training acc: 0.32598191499710083\n",
      "4440 - Training acc: 0.2953111529350281\n",
      "4450 - Training acc: 0.3331265449523926\n",
      "4460 - Training acc: 0.3303185999393463\n",
      "4470 - Training acc: 0.3349815905094147\n",
      "4480 - Training acc: 0.32830408215522766\n",
      "4490 - Training acc: 0.33756887912750244\n",
      "4500 - Training acc: 0.3334175944328308\n",
      "4510 - Training acc: 0.3494199812412262\n",
      "4520 - Training acc: 0.33494946360588074\n",
      "4530 - Training acc: 0.33235904574394226\n",
      "4540 - Training acc: 0.33745908737182617\n",
      "4550 - Training acc: 0.3290715515613556\n",
      "4560 - Training acc: 0.34058085083961487\n",
      "4570 - Training acc: 0.3323839008808136\n",
      "4580 - Training acc: 0.3364078104496002\n",
      "4590 - Training acc: 0.33949124813079834\n",
      "4600 - Training acc: 0.33157292008399963\n",
      "4610 - Training acc: 0.33960622549057007\n",
      "4620 - Training acc: 0.33342793583869934\n",
      "4630 - Training acc: 0.33427727222442627\n",
      "4640 - Training acc: 0.3394995331764221\n",
      "4650 - Training acc: 0.34054669737815857\n",
      "4660 - Training acc: 0.33400070667266846\n",
      "4670 - Training acc: 0.34335771203041077\n",
      "4680 - Training acc: 0.3449817895889282\n",
      "4690 - Training acc: 0.33872997760772705\n",
      "4700 - Training acc: 0.34148818254470825\n",
      "4710 - Training acc: 0.3438548743724823\n",
      "4720 - Training acc: 0.3445447087287903\n",
      "4730 - Training acc: 0.3328489661216736\n",
      "4740 - Training acc: 0.3288157284259796\n",
      "4750 - Training acc: 0.34373992681503296\n",
      "4760 - Training acc: 0.3460206389427185\n",
      "4770 - Training acc: 0.34010857343673706\n",
      "4780 - Training acc: 0.34378859400749207\n",
      "4790 - Training acc: 0.3538115918636322\n",
      "4800 - Training acc: 0.3468875586986542\n",
      "4810 - Training acc: 0.3480963110923767\n",
      "4820 - Training acc: 0.35245785117149353\n",
      "4830 - Training acc: 0.34123027324676514\n",
      "4840 - Training acc: 0.3410697281360626\n",
      "4850 - Training acc: 0.35211190581321716\n",
      "4860 - Training acc: 0.3475784361362457\n",
      "4870 - Training acc: 0.34268033504486084\n",
      "4880 - Training acc: 0.34636348485946655\n",
      "4890 - Training acc: 0.34623193740844727\n",
      "4900 - Training acc: 0.345770001411438\n",
      "4910 - Training acc: 0.3478384017944336\n",
      "4920 - Training acc: 0.3481905460357666\n",
      "4930 - Training acc: 0.3490305542945862\n",
      "4940 - Training acc: 0.34832313656806946\n",
      "4950 - Training acc: 0.3484556972980499\n",
      "4960 - Training acc: 0.3534967303276062\n",
      "4970 - Training acc: 0.3579680621623993\n",
      "4980 - Training acc: 0.34921181201934814\n",
      "4990 - Training acc: 0.35062041878700256\n",
      "5000 - Training acc: 0.3562135100364685\n",
      "5010 - Training acc: 0.35008183121681213\n",
      "5020 - Training acc: 0.3559058904647827\n",
      "5030 - Training acc: 0.35689711570739746\n",
      "5040 - Training acc: 0.3548535704612732\n",
      "5050 - Training acc: 0.3518519401550293\n",
      "5060 - Training acc: 0.3513123095035553\n",
      "5070 - Training acc: 0.35608920454978943\n",
      "5080 - Training acc: 0.3522983491420746\n",
      "5090 - Training acc: 0.3525231182575226\n",
      "5100 - Training acc: 0.3496406078338623\n",
      "5110 - Training acc: 0.353931725025177\n",
      "5120 - Training acc: 0.3539793789386749\n",
      "5130 - Training acc: 0.3580498993396759\n",
      "5140 - Training acc: 0.36066102981567383\n",
      "5150 - Training acc: 0.3580343723297119\n",
      "5160 - Training acc: 0.3588826358318329\n",
      "5170 - Training acc: 0.35502859950065613\n",
      "5180 - Training acc: 0.35788729786872864\n",
      "5190 - Training acc: 0.3619546890258789\n",
      "5200 - Training acc: 0.35969364643096924\n",
      "5210 - Training acc: 0.35964807868003845\n",
      "5220 - Training acc: 0.3618013858795166\n",
      "5230 - Training acc: 0.3582373559474945\n",
      "5240 - Training acc: 0.28312134742736816\n",
      "5250 - Training acc: 0.3414301872253418\n",
      "5260 - Training acc: 0.3327557444572449\n",
      "5270 - Training acc: 0.3251398503780365\n",
      "5280 - Training acc: 0.3223267197608948\n",
      "5290 - Training acc: 0.3409796357154846\n",
      "5300 - Training acc: 0.3591881990432739\n",
      "5310 - Training acc: 0.3442774713039398\n",
      "5320 - Training acc: 0.3442029058933258\n",
      "5330 - Training acc: 0.3398568630218506\n",
      "5340 - Training acc: 0.17301860451698303\n",
      "5350 - Training acc: 0.30087316036224365\n",
      "5360 - Training acc: 0.287826806306839\n",
      "5370 - Training acc: 0.30898207426071167\n",
      "5380 - Training acc: 0.2868904769420624\n",
      "5390 - Training acc: 0.355257511138916\n",
      "5400 - Training acc: 0.30816900730133057\n",
      "5410 - Training acc: 0.2972697615623474\n",
      "5420 - Training acc: 0.3445001542568207\n",
      "5430 - Training acc: 0.35456976294517517\n",
      "5440 - Training acc: 0.3017753064632416\n",
      "5450 - Training acc: 0.2703898549079895\n",
      "5460 - Training acc: 0.30061110854148865\n",
      "5470 - Training acc: 0.30832645297050476\n",
      "5480 - Training acc: 0.3052347004413605\n",
      "5490 - Training acc: 0.3458041846752167\n",
      "5500 - Training acc: 0.3527064323425293\n",
      "5510 - Training acc: 0.34376683831214905\n",
      "5520 - Training acc: 0.3148413300514221\n",
      "5530 - Training acc: 0.35583028197288513\n",
      "5540 - Training acc: 0.31238865852355957\n",
      "5550 - Training acc: 0.35729897022247314\n",
      "5560 - Training acc: 0.34975868463516235\n",
      "5570 - Training acc: 0.35957762598991394\n",
      "5580 - Training acc: 0.3626859188079834\n",
      "5590 - Training acc: 0.35244956612586975\n",
      "5600 - Training acc: 0.3615082800388336\n",
      "5610 - Training acc: 0.35899966955184937\n",
      "5620 - Training acc: 0.3541078269481659\n",
      "5630 - Training acc: 0.35894376039505005\n",
      "5640 - Training acc: 0.32488712668418884\n",
      "5650 - Training acc: 0.3531787395477295\n",
      "5660 - Training acc: 0.3602156639099121\n",
      "5670 - Training acc: 0.356966495513916\n",
      "5680 - Training acc: 0.3538457751274109\n",
      "5690 - Training acc: 0.3623710572719574\n",
      "5700 - Training acc: 0.35821977257728577\n",
      "5710 - Training acc: 0.3597557842731476\n",
      "5720 - Training acc: 0.35551127791404724\n",
      "5730 - Training acc: 0.3567676246166229\n",
      "5740 - Training acc: 0.3622768223285675\n",
      "5750 - Training acc: 0.3640230894088745\n",
      "5760 - Training acc: 0.36099350452423096\n",
      "5770 - Training acc: 0.3599194288253784\n",
      "5780 - Training acc: 0.36104634404182434\n",
      "5790 - Training acc: 0.358675479888916\n",
      "5800 - Training acc: 0.3539503812789917\n",
      "5810 - Training acc: 0.3573652505874634\n",
      "5820 - Training acc: 0.36306190490722656\n",
      "5830 - Training acc: 0.3620043992996216\n",
      "5840 - Training acc: 0.3613591194152832\n",
      "5850 - Training acc: 0.3622664511203766\n",
      "5860 - Training acc: 0.3569706380367279\n",
      "5870 - Training acc: 0.36372894048690796\n",
      "5880 - Training acc: 0.36063826084136963\n",
      "5890 - Training acc: 0.3630256652832031\n",
      "5900 - Training acc: 0.3632845878601074\n",
      "5910 - Training acc: 0.36291277408599854\n",
      "5920 - Training acc: 0.364402174949646\n",
      "5930 - Training acc: 0.3627159595489502\n",
      "5940 - Training acc: 0.3625326454639435\n",
      "5950 - Training acc: 0.3545283377170563\n",
      "5960 - Training acc: 0.3512895405292511\n",
      "5970 - Training acc: 0.3648061156272888\n",
      "5980 - Training acc: 0.32239094376564026\n",
      "5990 - Training acc: 0.35929280519485474\n",
      "6000 - Training acc: 0.35291773080825806\n",
      "6010 - Training acc: 0.3572741150856018\n",
      "6020 - Training acc: 0.3610069751739502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6030 - Training acc: 0.36406245827674866\n",
      "6040 - Training acc: 0.3638812005519867\n",
      "6050 - Training acc: 0.3632742464542389\n",
      "6060 - Training acc: 0.36437422037124634\n",
      "6070 - Training acc: 0.3640303313732147\n",
      "6080 - Training acc: 0.3591218888759613\n",
      "6090 - Training acc: 0.3626859188079834\n",
      "6100 - Training acc: 0.3617610037326813\n",
      "6110 - Training acc: 0.36361292004585266\n",
      "6120 - Training acc: 0.3281083106994629\n",
      "6130 - Training acc: 0.3605419099330902\n",
      "6140 - Training acc: 0.35807061195373535\n",
      "6150 - Training acc: 0.36347103118896484\n",
      "6160 - Training acc: 0.36350417137145996\n",
      "6170 - Training acc: 0.3632524907588959\n",
      "6180 - Training acc: 0.3631572127342224\n",
      "6190 - Training acc: 0.3617558181285858\n",
      "6200 - Training acc: 0.3529881536960602\n",
      "6210 - Training acc: 0.3623410165309906\n",
      "6220 - Training acc: 0.36293554306030273\n",
      "6230 - Training acc: 0.3636305332183838\n",
      "6240 - Training acc: 0.3602684736251831\n",
      "6250 - Training acc: 0.3653654158115387\n",
      "6260 - Training acc: 0.362265408039093\n",
      "6270 - Training acc: 0.3525790572166443\n",
      "6280 - Training acc: 0.36512720584869385\n",
      "6290 - Training acc: 0.36433693766593933\n",
      "6300 - Training acc: 0.364380419254303\n",
      "6310 - Training acc: 0.3623824715614319\n",
      "6320 - Training acc: 0.35661745071411133\n",
      "6330 - Training acc: 0.3637869358062744\n",
      "6340 - Training acc: 0.3643752634525299\n",
      "6350 - Training acc: 0.3632100224494934\n",
      "6360 - Training acc: 0.3619546890258789\n",
      "6370 - Training acc: 0.36531156301498413\n",
      "6380 - Training acc: 0.36036378145217896\n",
      "6390 - Training acc: 0.3633374273777008\n",
      "6400 - Training acc: 0.3639329671859741\n",
      "6410 - Training acc: 0.3631323575973511\n",
      "6420 - Training acc: 0.362730473279953\n",
      "6430 - Training acc: 0.32539981603622437\n",
      "6440 - Training acc: 0.36137673258781433\n",
      "6450 - Training acc: 0.31940382719039917\n",
      "6460 - Training acc: 0.3621711730957031\n",
      "6470 - Training acc: 0.3599277138710022\n",
      "6480 - Training acc: 0.36406970024108887\n",
      "6490 - Training acc: 0.35802608728408813\n",
      "6500 - Training acc: 0.36388638615608215\n",
      "6510 - Training acc: 0.365774542093277\n",
      "6520 - Training acc: 0.35105544328689575\n",
      "6530 - Training acc: 0.3629666268825531\n",
      "6540 - Training acc: 0.36585742235183716\n",
      "6550 - Training acc: 0.3570897579193115\n",
      "6560 - Training acc: 0.3659640848636627\n",
      "6570 - Training acc: 0.36363157629966736\n",
      "6580 - Training acc: 0.324813574552536\n",
      "6590 - Training acc: 0.3642074465751648\n",
      "6600 - Training acc: 0.3630950450897217\n",
      "6610 - Training acc: 0.36273667216300964\n",
      "6620 - Training acc: 0.3652152419090271\n",
      "6630 - Training acc: 0.3552657961845398\n",
      "6640 - Training acc: 0.36640220880508423\n",
      "6650 - Training acc: 0.3631810247898102\n",
      "6660 - Training acc: 0.3583917021751404\n",
      "6670 - Training acc: 0.3648972809314728\n",
      "6680 - Training acc: 0.3577205240726471\n",
      "6690 - Training acc: 0.36610081791877747\n",
      "6700 - Training acc: 0.3650474548339844\n",
      "6710 - Training acc: 0.36320382356643677\n",
      "6720 - Training acc: 0.36402517557144165\n",
      "6730 - Training acc: 0.3643193244934082\n",
      "6740 - Training acc: 0.36578282713890076\n",
      "6750 - Training acc: 0.3631572127342224\n",
      "6760 - Training acc: 0.3531942665576935\n",
      "6770 - Training acc: 0.36654308438301086\n",
      "6780 - Training acc: 0.3568660318851471\n",
      "6790 - Training acc: 0.35829225182533264\n",
      "6800 - Training acc: 0.3639785647392273\n",
      "6810 - Training acc: 0.3624798059463501\n",
      "6820 - Training acc: 0.33185669779777527\n",
      "6830 - Training acc: 0.3652110993862152\n",
      "6840 - Training acc: 0.35201042890548706\n",
      "6850 - Training acc: 0.36481958627700806\n",
      "6860 - Training acc: 0.36336538195610046\n",
      "6870 - Training acc: 0.34730809926986694\n",
      "6880 - Training acc: 0.3649677038192749\n",
      "6890 - Training acc: 0.3654058277606964\n",
      "6900 - Training acc: 0.3617154359817505\n",
      "6910 - Training acc: 0.36310437321662903\n",
      "6920 - Training acc: 0.35803642868995667\n",
      "6930 - Training acc: 0.36144405603408813\n",
      "6940 - Training acc: 0.36598894000053406\n",
      "6950 - Training acc: 0.36071282625198364\n",
      "6960 - Training acc: 0.3646238148212433\n",
      "6970 - Training acc: 0.36651304364204407\n",
      "6980 - Training acc: 0.36335399746894836\n",
      "6990 - Training acc: 0.36307641863822937\n",
      "7000 - Training acc: 0.36531364917755127\n",
      "7010 - Training acc: 0.3638801574707031\n",
      "7020 - Training acc: 0.36526599526405334\n",
      "7030 - Training acc: 0.3644498288631439\n",
      "7040 - Training acc: 0.3650195002555847\n",
      "7050 - Training acc: 0.36483824253082275\n",
      "7060 - Training acc: 0.36218875646591187\n",
      "7070 - Training acc: 0.3639070987701416\n",
      "7080 - Training acc: 0.36462175846099854\n",
      "7090 - Training acc: 0.3599453270435333\n",
      "7100 - Training acc: 0.3564020097255707\n",
      "7110 - Training acc: 0.3663245439529419\n",
      "7120 - Training acc: 0.3653229773044586\n",
      "7130 - Training acc: 0.3659278452396393\n",
      "7140 - Training acc: 0.3645585775375366\n",
      "7150 - Training acc: 0.3617516756057739\n",
      "7160 - Training acc: 0.36739033460617065\n",
      "7170 - Training acc: 0.3612089455127716\n",
      "7180 - Training acc: 0.36453062295913696\n",
      "7190 - Training acc: 0.3658263385295868\n",
      "7200 - Training acc: 0.36469218134880066\n",
      "7210 - Training acc: 0.33133572340011597\n",
      "7220 - Training acc: 0.365278422832489\n",
      "7230 - Training acc: 0.3651323914527893\n",
      "7240 - Training acc: 0.3641504943370819\n",
      "7250 - Training acc: 0.36338508129119873\n",
      "7260 - Training acc: 0.3646155297756195\n",
      "7270 - Training acc: 0.365577757358551\n",
      "7280 - Training acc: 0.3653623163700104\n",
      "7290 - Training acc: 0.36514171957969666\n",
      "7300 - Training acc: 0.3639526665210724\n",
      "7310 - Training acc: 0.3652069568634033\n",
      "7320 - Training acc: 0.3651779592037201\n",
      "7330 - Training acc: 0.3671489953994751\n",
      "7340 - Training acc: 0.3644953966140747\n",
      "7350 - Training acc: 0.3627781271934509\n",
      "7360 - Training acc: 0.3666549324989319\n",
      "7370 - Training acc: 0.36630383133888245\n",
      "7380 - Training acc: 0.3646901249885559\n",
      "7390 - Training acc: 0.36652132868766785\n",
      "7400 - Training acc: 0.3639153838157654\n",
      "7410 - Training acc: 0.36272838711738586\n",
      "7420 - Training acc: 0.365016371011734\n",
      "7430 - Training acc: 0.366335928440094\n",
      "7440 - Training acc: 0.36403656005859375\n",
      "7450 - Training acc: 0.3602498471736908\n",
      "7460 - Training acc: 0.3653167486190796\n",
      "7470 - Training acc: 0.3653944432735443\n",
      "7480 - Training acc: 0.3670899569988251\n",
      "7490 - Training acc: 0.36565855145454407\n",
      "7500 - Training acc: 0.36412978172302246\n",
      "7510 - Training acc: 0.3642478585243225\n",
      "7520 - Training acc: 0.36634111404418945\n",
      "7530 - Training acc: 0.36263206601142883\n",
      "7540 - Training acc: 0.36614638566970825\n",
      "7550 - Training acc: 0.36498531699180603\n",
      "7560 - Training acc: 0.3661101460456848\n",
      "7570 - Training acc: 0.36323487758636475\n",
      "7580 - Training acc: 0.3649646043777466\n",
      "7590 - Training acc: 0.3645969033241272\n",
      "7600 - Training acc: 0.36203134059906006\n",
      "7610 - Training acc: 0.3671873211860657\n",
      "7620 - Training acc: 0.36593613028526306\n",
      "7630 - Training acc: 0.3517991304397583\n",
      "7640 - Training acc: 0.36517587304115295\n",
      "7650 - Training acc: 0.36680513620376587\n",
      "7660 - Training acc: 0.35965222120285034\n",
      "7670 - Training acc: 0.36399099230766296\n",
      "7680 - Training acc: 0.36608630418777466\n",
      "7690 - Training acc: 0.36675748229026794\n",
      "7700 - Training acc: 0.3677228093147278\n",
      "7710 - Training acc: 0.356518030166626\n",
      "7720 - Training acc: 0.3671034276485443\n",
      "7730 - Training acc: 0.3663918673992157\n",
      "7740 - Training acc: 0.36680200695991516\n",
      "7750 - Training acc: 0.36424991488456726\n",
      "7760 - Training acc: 0.36551252007484436\n",
      "7770 - Training acc: 0.3669915497303009\n",
      "7780 - Training acc: 0.3658698499202728\n",
      "7790 - Training acc: 0.3672235608100891\n",
      "7800 - Training acc: 0.3681339919567108\n",
      "7810 - Training acc: 0.36276569962501526\n",
      "7820 - Training acc: 0.36776837706565857\n",
      "7830 - Training acc: 0.36736753582954407\n",
      "7840 - Training acc: 0.36677923798561096\n",
      "7850 - Training acc: 0.36802005767822266\n",
      "7860 - Training acc: 0.3667067289352417\n",
      "7870 - Training acc: 0.3681495487689972\n",
      "7880 - Training acc: 0.366971880197525\n",
      "7890 - Training acc: 0.3659154176712036\n",
      "7900 - Training acc: 0.3663659691810608\n",
      "7910 - Training acc: 0.3678377866744995\n",
      "7920 - Training acc: 0.3679475784301758\n",
      "7930 - Training acc: 0.36520281434059143\n",
      "7940 - Training acc: 0.36700916290283203\n",
      "7950 - Training acc: 0.3639526665210724\n",
      "7960 - Training acc: 0.36856281757354736\n",
      "7970 - Training acc: 0.3648371994495392\n",
      "7980 - Training acc: 0.36896881461143494\n",
      "7990 - Training acc: 0.3669698238372803\n",
      "8000 - Training acc: 0.36581185460090637\n",
      "8010 - Training acc: 0.368007630109787\n",
      "8020 - Training acc: 0.3627221882343292\n",
      "8030 - Training acc: 0.3688051700592041\n",
      "8040 - Training acc: 0.36798587441444397\n",
      "8050 - Training acc: 0.3678087592124939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8060 - Training acc: 0.36804699897766113\n",
      "8070 - Training acc: 0.3612907826900482\n",
      "8080 - Training acc: 0.36709100008010864\n",
      "8090 - Training acc: 0.3682582974433899\n",
      "8100 - Training acc: 0.36838775873184204\n",
      "8110 - Training acc: 0.36873161792755127\n",
      "8120 - Training acc: 0.3687274754047394\n",
      "8130 - Training acc: 0.3684975504875183\n",
      "8140 - Training acc: 0.36819717288017273\n",
      "8150 - Training acc: 0.36870884895324707\n",
      "8160 - Training acc: 0.36755189299583435\n",
      "8170 - Training acc: 0.37375712394714355\n",
      "8180 - Training acc: 0.36560365557670593\n",
      "8190 - Training acc: 0.36744004487991333\n",
      "8200 - Training acc: 0.3689097762107849\n",
      "8210 - Training acc: 0.3678305149078369\n",
      "8220 - Training acc: 0.3672422170639038\n",
      "8230 - Training acc: 0.3629935383796692\n",
      "8240 - Training acc: 0.36678647994995117\n",
      "8250 - Training acc: 0.3689180612564087\n",
      "8260 - Training acc: 0.36732715368270874\n",
      "8270 - Training acc: 0.3686860501766205\n",
      "8280 - Training acc: 0.3642737567424774\n",
      "8290 - Training acc: 0.3686560094356537\n",
      "8300 - Training acc: 0.368216872215271\n",
      "8310 - Training acc: 0.360519140958786\n",
      "8320 - Training acc: 0.36962756514549255\n",
      "8330 - Training acc: 0.3680117726325989\n",
      "8340 - Training acc: 0.3648744821548462\n",
      "8350 - Training acc: 0.3767763376235962\n",
      "8360 - Training acc: 0.3697798252105713\n",
      "8370 - Training acc: 0.36560365557670593\n",
      "8380 - Training acc: 0.3680967092514038\n",
      "8390 - Training acc: 0.3689501881599426\n",
      "8400 - Training acc: 0.3661339581012726\n",
      "8410 - Training acc: 0.3687761723995209\n",
      "8420 - Training acc: 0.3683970868587494\n",
      "8430 - Training acc: 0.3247731924057007\n",
      "8440 - Training acc: 0.33537930250167847\n",
      "8450 - Training acc: 0.32490989565849304\n",
      "8460 - Training acc: 0.33997392654418945\n",
      "8470 - Training acc: 0.3239860236644745\n",
      "8480 - Training acc: 0.32875359058380127\n",
      "8490 - Training acc: 0.3525417447090149\n",
      "8500 - Training acc: 0.36018872261047363\n",
      "8510 - Training acc: 0.3646528422832489\n",
      "8520 - Training acc: 0.36538925766944885\n",
      "8530 - Training acc: 0.36683619022369385\n",
      "8540 - Training acc: 0.3675415515899658\n",
      "8550 - Training acc: 0.36801591515541077\n",
      "8560 - Training acc: 0.36896467208862305\n",
      "8570 - Training acc: 0.3634006083011627\n",
      "8580 - Training acc: 0.3686259984970093\n",
      "8590 - Training acc: 0.3025054931640625\n",
      "8600 - Training acc: 0.3329121470451355\n",
      "8610 - Training acc: 0.3307577669620514\n",
      "8620 - Training acc: 0.33240774273872375\n",
      "8630 - Training acc: 0.34299832582473755\n",
      "8640 - Training acc: 0.34247317910194397\n",
      "8650 - Training acc: 0.34584975242614746\n",
      "8660 - Training acc: 0.3480486571788788\n",
      "8670 - Training acc: 0.31271597743034363\n",
      "8680 - Training acc: 0.3460610508918762\n",
      "8690 - Training acc: 0.35548433661460876\n",
      "8700 - Training acc: 0.3546391725540161\n",
      "8710 - Training acc: 0.3561192452907562\n",
      "8720 - Training acc: 0.35723578929901123\n",
      "8730 - Training acc: 0.3621007204055786\n",
      "8740 - Training acc: 0.3518995940685272\n",
      "8750 - Training acc: 0.362714946269989\n",
      "8760 - Training acc: 0.3642923831939697\n",
      "8770 - Training acc: 0.35430875420570374\n",
      "8780 - Training acc: 0.36410802602767944\n",
      "8790 - Training acc: 0.36031922698020935\n",
      "8800 - Training acc: 0.3665409982204437\n",
      "8810 - Training acc: 0.36320897936820984\n",
      "8820 - Training acc: 0.35986247658729553\n",
      "8830 - Training acc: 0.34980735182762146\n",
      "8840 - Training acc: 0.32350853085517883\n",
      "8850 - Training acc: 0.36420539021492004\n",
      "8860 - Training acc: 0.36518415808677673\n",
      "8870 - Training acc: 0.3593880832195282\n",
      "8880 - Training acc: 0.3649449050426483\n",
      "8890 - Training acc: 0.36405518651008606\n",
      "8900 - Training acc: 0.36477091908454895\n",
      "8910 - Training acc: 0.36542758345603943\n",
      "8920 - Training acc: 0.37255045771598816\n",
      "8930 - Training acc: 0.3642706274986267\n",
      "8940 - Training acc: 0.3728259801864624\n",
      "8950 - Training acc: 0.36567822098731995\n",
      "8960 - Training acc: 0.36408731341362\n",
      "8970 - Training acc: 0.37262606620788574\n",
      "8980 - Training acc: 0.36806565523147583\n",
      "8990 - Training acc: 0.3658905625343323\n",
      "9000 - Training acc: 0.3725142180919647\n",
      "9010 - Training acc: 0.3728269934654236\n",
      "9020 - Training acc: 0.37234020233154297\n",
      "9030 - Training acc: 0.3690195679664612\n",
      "9040 - Training acc: 0.3723433017730713\n",
      "9050 - Training acc: 0.3733169138431549\n",
      "9060 - Training acc: 0.37263745069503784\n",
      "9070 - Training acc: 0.3643234670162201\n",
      "9080 - Training acc: 0.3737260401248932\n",
      "9090 - Training acc: 0.37437546253204346\n",
      "9100 - Training acc: 0.37584105134010315\n",
      "9110 - Training acc: 0.3746219575405121\n",
      "9120 - Training acc: 0.3711656630039215\n",
      "9130 - Training acc: 0.3731346130371094\n",
      "9140 - Training acc: 0.3745691478252411\n",
      "9150 - Training acc: 0.3685928285121918\n",
      "9160 - Training acc: 0.3746292293071747\n",
      "9170 - Training acc: 0.365802526473999\n",
      "9180 - Training acc: 0.36754465103149414\n",
      "9190 - Training acc: 0.37182751297950745\n",
      "9200 - Training acc: 0.37395909428596497\n",
      "9210 - Training acc: 0.3749358057975769\n",
      "9220 - Training acc: 0.3737633228302002\n",
      "9230 - Training acc: 0.365256667137146\n",
      "9240 - Training acc: 0.3741527795791626\n",
      "9250 - Training acc: 0.36670467257499695\n",
      "9260 - Training acc: 0.3748115003108978\n",
      "9270 - Training acc: 0.3678988814353943\n",
      "9280 - Training acc: 0.3737705647945404\n",
      "9290 - Training acc: 0.374429315328598\n",
      "9300 - Training acc: 0.37346503138542175\n",
      "9310 - Training acc: 0.3749202489852905\n",
      "9320 - Training acc: 0.37450700998306274\n",
      "9330 - Training acc: 0.3727482855319977\n",
      "9340 - Training acc: 0.3679890036582947\n",
      "9350 - Training acc: 0.37472450733184814\n",
      "9360 - Training acc: 0.37434956431388855\n",
      "9370 - Training acc: 0.37440237402915955\n",
      "9380 - Training acc: 0.37577372789382935\n",
      "9390 - Training acc: 0.37176328897476196\n",
      "9400 - Training acc: 0.3734795153141022\n",
      "9410 - Training acc: 0.3735975921154022\n",
      "9420 - Training acc: 0.37556037306785583\n",
      "9430 - Training acc: 0.375021755695343\n",
      "9440 - Training acc: 0.37652361392974854\n",
      "9450 - Training acc: 0.3743371367454529\n",
      "9460 - Training acc: 0.375021755695343\n",
      "9470 - Training acc: 0.374033659696579\n",
      "9480 - Training acc: 0.37573954463005066\n",
      "9490 - Training acc: 0.3735240697860718\n",
      "9500 - Training acc: 0.375412255525589\n",
      "9510 - Training acc: 0.37602749466896057\n",
      "9520 - Training acc: 0.37573644518852234\n",
      "9530 - Training acc: 0.3763299286365509\n",
      "9540 - Training acc: 0.3772672712802887\n",
      "9550 - Training acc: 0.3751574456691742\n",
      "9560 - Training acc: 0.376251220703125\n",
      "9570 - Training acc: 0.37581825256347656\n",
      "9580 - Training acc: 0.3768633306026459\n",
      "9590 - Training acc: 0.3761579990386963\n",
      "9600 - Training acc: 0.3763009309768677\n",
      "9610 - Training acc: 0.3762367069721222\n",
      "9620 - Training acc: 0.37584415078163147\n",
      "9630 - Training acc: 0.3713686764240265\n",
      "9640 - Training acc: 0.37632474303245544\n",
      "9650 - Training acc: 0.3683411478996277\n",
      "9660 - Training acc: 0.37634027004241943\n",
      "9670 - Training acc: 0.3655715584754944\n",
      "9680 - Training acc: 0.3763330280780792\n",
      "9690 - Training acc: 0.3769006133079529\n",
      "9700 - Training acc: 0.37611448764801025\n",
      "9710 - Training acc: 0.3756701350212097\n",
      "9720 - Training acc: 0.37694621086120605\n",
      "9730 - Training acc: 0.37672868371009827\n",
      "9740 - Training acc: 0.3760564923286438\n",
      "9750 - Training acc: 0.375449538230896\n",
      "9760 - Training acc: 0.3765505254268646\n",
      "9770 - Training acc: 0.37678876519203186\n",
      "9780 - Training acc: 0.37565356492996216\n",
      "9790 - Training acc: 0.37690269947052\n",
      "9800 - Training acc: 0.37647905945777893\n",
      "9810 - Training acc: 0.37669864296913147\n",
      "9820 - Training acc: 0.37659403681755066\n",
      "9830 - Training acc: 0.3750590682029724\n",
      "9840 - Training acc: 0.3758949041366577\n",
      "9850 - Training acc: 0.3767721951007843\n",
      "9860 - Training acc: 0.37720823287963867\n",
      "9870 - Training acc: 0.3709926903247833\n",
      "9880 - Training acc: 0.376769095659256\n",
      "9890 - Training acc: 0.3768094778060913\n",
      "9900 - Training acc: 0.3762781322002411\n",
      "9910 - Training acc: 0.3771326243877411\n",
      "9920 - Training acc: 0.3768664598464966\n",
      "9930 - Training acc: 0.3768778443336487\n",
      "9940 - Training acc: 0.37639620900154114\n",
      "9950 - Training acc: 0.37656712532043457\n",
      "9960 - Training acc: 0.37702491879463196\n",
      "9970 - Training acc: 0.37571054697036743\n",
      "9980 - Training acc: 0.37739571928977966\n",
      "9990 - Training acc: 0.37690579891204834\n",
      "10000 - Training acc: 0.37633925676345825\n",
      "<class 'torch.Tensor'>\n",
      "965480\n",
      "413778\n"
     ]
    }
   ],
   "source": [
    "cname_label = 'Attack'\n",
    "ton_multi_prob_df = run_baseline(\n",
    "    ds_name,\n",
    "    cname_label,\n",
    "    n_epochs\n",
    ")\n",
    "ton_multi_prob_df.to_csv('../output/EGraphSAGE_nf_ton_multi.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36c5d196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3254e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f2ce9c0",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c61c893",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600100, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IPV4_SRC_ADDR</th>\n",
       "      <th>L4_SRC_PORT</th>\n",
       "      <th>IPV4_DST_ADDR</th>\n",
       "      <th>L4_DST_PORT</th>\n",
       "      <th>PROTOCOL</th>\n",
       "      <th>L7_PROTO</th>\n",
       "      <th>IN_BYTES</th>\n",
       "      <th>OUT_BYTES</th>\n",
       "      <th>IN_PKTS</th>\n",
       "      <th>OUT_PKTS</th>\n",
       "      <th>TCP_FLAGS</th>\n",
       "      <th>FLOW_DURATION_MILLISECONDS</th>\n",
       "      <th>Label</th>\n",
       "      <th>Attack</th>\n",
       "      <th>Label_tvt</th>\n",
       "      <th>Attack_tvt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>192.168.100.6</td>\n",
       "      <td>52670</td>\n",
       "      <td>192.168.100.1</td>\n",
       "      <td>53</td>\n",
       "      <td>17</td>\n",
       "      <td>5.212</td>\n",
       "      <td>71</td>\n",
       "      <td>126</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4294966</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>192.168.100.6</td>\n",
       "      <td>49160</td>\n",
       "      <td>192.168.100.149</td>\n",
       "      <td>4444</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000</td>\n",
       "      <td>217753000</td>\n",
       "      <td>199100</td>\n",
       "      <td>4521</td>\n",
       "      <td>4049</td>\n",
       "      <td>24</td>\n",
       "      <td>4176249</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>test</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>192.168.100.46</td>\n",
       "      <td>3456</td>\n",
       "      <td>192.168.100.5</td>\n",
       "      <td>80</td>\n",
       "      <td>17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8508021</td>\n",
       "      <td>8918372</td>\n",
       "      <td>9086</td>\n",
       "      <td>9086</td>\n",
       "      <td>0</td>\n",
       "      <td>4175916</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>192.168.100.3</td>\n",
       "      <td>80</td>\n",
       "      <td>192.168.100.55</td>\n",
       "      <td>8080</td>\n",
       "      <td>6</td>\n",
       "      <td>7.000</td>\n",
       "      <td>8442138</td>\n",
       "      <td>9013406</td>\n",
       "      <td>9086</td>\n",
       "      <td>9086</td>\n",
       "      <td>0</td>\n",
       "      <td>4175916</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>192.168.100.46</td>\n",
       "      <td>80</td>\n",
       "      <td>192.168.100.5</td>\n",
       "      <td>80</td>\n",
       "      <td>6</td>\n",
       "      <td>7.000</td>\n",
       "      <td>8374706</td>\n",
       "      <td>0</td>\n",
       "      <td>9086</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4175916</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    IPV4_SRC_ADDR  L4_SRC_PORT    IPV4_DST_ADDR  L4_DST_PORT  PROTOCOL  \\\n",
       "0   192.168.100.6        52670    192.168.100.1           53        17   \n",
       "1   192.168.100.6        49160  192.168.100.149         4444         6   \n",
       "2  192.168.100.46         3456    192.168.100.5           80        17   \n",
       "3   192.168.100.3           80   192.168.100.55         8080         6   \n",
       "4  192.168.100.46           80    192.168.100.5           80         6   \n",
       "\n",
       "   L7_PROTO   IN_BYTES  OUT_BYTES  IN_PKTS  OUT_PKTS  TCP_FLAGS  \\\n",
       "0     5.212         71        126        1         1          0   \n",
       "1     0.000  217753000     199100     4521      4049         24   \n",
       "2     0.000    8508021    8918372     9086      9086          0   \n",
       "3     7.000    8442138    9013406     9086      9086          0   \n",
       "4     7.000    8374706          0     9086         0          0   \n",
       "\n",
       "   FLOW_DURATION_MILLISECONDS  Label  Attack Label_tvt Attack_tvt  \n",
       "0                     4294966      0       0     train      train  \n",
       "1                     4176249      1       4      test       test  \n",
       "2                     4175916      0       0     train      train  \n",
       "3                     4175916      0       0     train      train  \n",
       "4                     4175916      0       0     train      train  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_name = 'NF-BoT-IoT'\n",
    "cname_label = 'Label'\n",
    "cname_tvt = f'{cname_label}_tvt'\n",
    "n_epochs = 10\n",
    "device = 'cuda:0'\n",
    "\n",
    "data = pd.read_csv(f'../datasets/{ds_name}_tvt.csv')\n",
    "label2idx = pd.read_pickle(f'../datasets/{ds_name}_graph_multi.pkl')['label2idx']\n",
    "data['Attack'] = data['Attack'].map(label2idx)\n",
    "\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce4fcb9f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data['IPV4_SRC_ADDR'] = data.IPV4_SRC_ADDR.apply(\n",
    "    lambda x: socket.inet_ntoa(struct.pack('>I', random.randint(0xac100001, 0xac1f0001))))\n",
    "data['IPV4_SRC_ADDR'] = data.IPV4_SRC_ADDR.apply(str)\n",
    "data['L4_SRC_PORT'] = data.L4_SRC_PORT.apply(str)\n",
    "data['IPV4_DST_ADDR'] = data.IPV4_DST_ADDR.apply(str)\n",
    "data['L4_DST_PORT'] = data.L4_DST_PORT.apply(str)\n",
    "\n",
    "data['IPV4_SRC_ADDR'] = data['IPV4_SRC_ADDR'] + ':' + data['L4_SRC_PORT']\n",
    "data['IPV4_DST_ADDR'] = data['IPV4_DST_ADDR'] + ':' + data['L4_DST_PORT']\n",
    "\n",
    "data.drop(columns=['L4_SRC_PORT','L4_DST_PORT'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5dd37c89",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_cnames = [c for c in data.columns if c not in ['Label_tvt', 'Attack_tvt']]\n",
    "X_train, X_test, y_train, y_test = (data[data[cname_tvt]!='test'][X_cnames], \n",
    "                                    data[data[cname_tvt]=='test'][X_cnames], \n",
    "                                    data[data[cname_tvt]!='test'][cname_label], \n",
    "                                    data[data[cname_tvt]=='test'][cname_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fe4520bb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_graph(scaler, encoder, X, y, cols_to_norm):\n",
    "    X = encoder.transform(X)\n",
    "    print(cols_to_norm)\n",
    "    X[cols_to_norm] = scaler.transform(X[cols_to_norm])\n",
    "    X['h'] = X[cols_to_norm].values.tolist()\n",
    "    X['h'] = X['h'].apply(lambda x: torch.tensor(x))\n",
    "    print(X, X.shape)\n",
    "    G = nx.from_pandas_edgelist(\n",
    "        X, \"IPV4_SRC_ADDR\", \"IPV4_DST_ADDR\", ['h', cname_label], create_using=nx.MultiGraph())\n",
    "    G = G.to_directed()\n",
    "    print(G.number_of_nodes())\n",
    "    print(G.number_of_edges())\n",
    "    G = from_networkx(G, edge_attrs=['h', cname_label])\n",
    "    print('G nodes', G.number_of_nodes())\n",
    "    print('G edges', G.number_of_edges())\n",
    "    # Eq1\n",
    "    G.ndata['h'] = torch.ones(G.num_nodes(), G.edata['h'].shape[1])\n",
    "    G.edata['train_mask'] = torch.ones(len(G.edata['h']), dtype=torch.bool)\n",
    "    print('G ndata', G.ndata['h'].shape)\n",
    "    print('G edata', G.edata['train_mask'].shape)\n",
    "    \n",
    "    G.ndata['h'] = torch.reshape(G.ndata['h'], (G.ndata['h'].shape[0], 1, G.ndata['h'].shape[1]))\n",
    "    G.edata['h'] = torch.reshape(G.edata['h'], (G.edata['h'].shape[0], 1, G.edata['h'].shape[1]))\n",
    "    print('G ndata', G.ndata['h'].shape)\n",
    "    print('G edata', G.edata['h'].shape)\n",
    "    G = G.to(device)\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a8cd3cdd",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OUT_BYTES', 'IN_PKTS', 'OUT_PKTS', 'TCP_FLAGS', 'PROTOCOL', 'L7_PROTO', 'FLOW_DURATION_MILLISECONDS', 'IN_BYTES']\n",
      "               IPV4_SRC_ADDR         IPV4_DST_ADDR  PROTOCOL  L7_PROTO  \\\n",
      "0       172.16.165.142:52670      192.168.100.1:53 -2.268430 -0.255669   \n",
      "2        172.19.188.130:3456      192.168.100.5:80 -2.268430 -0.227547   \n",
      "3          172.30.194.190:80   192.168.100.55:8080 -2.177751 -0.227166   \n",
      "4            172.19.59.45:80      192.168.100.5:80 -2.177751 -0.227166   \n",
      "5           172.29.116.152:0       192.168.100.3:0 -2.177751 -0.227547   \n",
      "...                      ...                   ...       ...       ...   \n",
      "600091    172.28.202.83:3456      192.168.100.5:80 -2.268430 -0.227547   \n",
      "600092   172.29.191.120:8080      192.168.100.3:80 -2.177751 -0.227166   \n",
      "600094      172.27.119.97:80      192.168.100.3:80 -2.177751 -0.227166   \n",
      "600095     172.19.103.242:80      192.168.100.5:80 -2.177751 -0.227166   \n",
      "600099  172.29.164.238:49160  192.168.100.149:4444 -2.177751 -0.227547   \n",
      "\n",
      "         IN_BYTES  OUT_BYTES    IN_PKTS   OUT_PKTS  TCP_FLAGS  \\\n",
      "0       -0.020609  -0.008339  -0.045684  -0.023745  -2.588792   \n",
      "2       19.653852  12.195069  36.833188  43.911817  -2.588792   \n",
      "3       19.501499  12.325110  36.833188  43.911817  -2.588792   \n",
      "4       19.345563  -0.008511  36.833188  -0.028581  -2.588792   \n",
      "5        8.764996  -0.008511  22.081639  -0.028581  -2.588792   \n",
      "...           ...        ...        ...        ...        ...   \n",
      "600091   5.441294   3.379776  10.191907  12.172786  -2.588792   \n",
      "600092   5.764577   3.199914  10.191907  12.172786  -2.588792   \n",
      "600094   0.534832  -0.008511   3.408792  -0.028581  -2.588792   \n",
      "600095   5.367455  -0.008511  10.191907  -0.028581  -2.588792   \n",
      "600099  92.715023   0.042501   3.047513   2.824691  -2.667136   \n",
      "\n",
      "        FLOW_DURATION_MILLISECONDS  Label  Attack  \\\n",
      "0                         0.496526      0       0   \n",
      "2                         0.425057      0       0   \n",
      "3                         0.425057      0       0   \n",
      "4                         0.425057      0       0   \n",
      "5                         0.425067      0       0   \n",
      "...                            ...    ...     ...   \n",
      "600091                    0.477358      0       0   \n",
      "600092                    0.477358      0       0   \n",
      "600094                    0.477380      0       0   \n",
      "600095                    0.477358      0       0   \n",
      "600099                    0.481579      1       4   \n",
      "\n",
      "                                                        h  \n",
      "0       [tensor(-0.0083), tensor(-0.0457), tensor(-0.0...  \n",
      "2       [tensor(12.1951), tensor(36.8332), tensor(43.9...  \n",
      "3       [tensor(12.3251), tensor(36.8332), tensor(43.9...  \n",
      "4       [tensor(-0.0085), tensor(36.8332), tensor(-0.0...  \n",
      "5       [tensor(-0.0085), tensor(22.0816), tensor(-0.0...  \n",
      "...                                                   ...  \n",
      "600091  [tensor(3.3798), tensor(10.1919), tensor(12.17...  \n",
      "600092  [tensor(3.1999), tensor(10.1919), tensor(12.17...  \n",
      "600094  [tensor(-0.0085), tensor(3.4088), tensor(-0.02...  \n",
      "600095  [tensor(-0.0085), tensor(10.1919), tensor(-0.0...  \n",
      "600099  [tensor(0.0425), tensor(3.0475), tensor(2.8247...  \n",
      "\n",
      "[420070 rows x 13 columns] (420070, 13)\n",
      "433037\n",
      "840140\n",
      "G nodes 433037\n",
      "G edges 840140\n",
      "G ndata torch.Size([433037, 8])\n",
      "G edata torch.Size([840140])\n",
      "G ndata torch.Size([433037, 1, 8])\n",
      "G edata torch.Size([840140, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "G_train = build_graph(scaler, encoder, X_train, y_train, cols_to_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "299e6b21",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "840140"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_train.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9e1aa028",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "420070"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5117641a",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(420070, 13)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0d2fb346",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OUT_BYTES', 'IN_PKTS', 'OUT_PKTS', 'TCP_FLAGS', 'PROTOCOL', 'L7_PROTO', 'FLOW_DURATION_MILLISECONDS', 'IN_BYTES']\n"
     ]
    }
   ],
   "source": [
    "X = encoder.transform(X_train)\n",
    "print(cols_to_norm)\n",
    "X[cols_to_norm] = scaler.transform(X[cols_to_norm])\n",
    "X['h'] = X[cols_to_norm].values.tolist()\n",
    "X['h'] = X['h'].apply(lambda x: torch.tensor(x))\n",
    "G = nx.from_pandas_edgelist(\n",
    "    X, \"IPV4_SRC_ADDR\", \"IPV4_DST_ADDR\", ['h', cname_label], create_using=nx.DiGraph())\n",
    "# G = G.to_directed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d64f3f0e",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/miniconda2/envs/iot/lib/python3.9/site-packages/category_encoders/target_encoder.py:92: FutureWarning: Default parameter min_samples_leaf will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter min_samples_leaf will change in version 2.6.\"\n",
      "/home/hoang/miniconda2/envs/iot/lib/python3.9/site-packages/category_encoders/target_encoder.py:97: FutureWarning: Default parameter smoothing will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter smoothing will change in version 2.6.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OUT_BYTES', 'IN_PKTS', 'OUT_PKTS', 'TCP_FLAGS', 'PROTOCOL', 'L7_PROTO', 'FLOW_DURATION_MILLISECONDS', 'IN_BYTES']\n",
      "['OUT_BYTES', 'IN_PKTS', 'OUT_PKTS', 'TCP_FLAGS', 'PROTOCOL', 'L7_PROTO', 'FLOW_DURATION_MILLISECONDS', 'IN_BYTES']\n"
     ]
    }
   ],
   "source": [
    "cols_to_norm = list(set(X_train.columns) - set(['Label', 'Attack', 'IPV4_SRC_ADDR', 'IPV4_DST_ADDR']))\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train[cols_to_norm])\n",
    "encoder = ce.TargetEncoder(cols=['TCP_FLAGS','L7_PROTO','PROTOCOL'])\n",
    "encoder.fit(X_train, y_train)\n",
    "\n",
    "G_train = build_graph(scaler, encoder, X_train, y_train, cols_to_norm)\n",
    "G_test = build_graph(scaler, encoder, X_test, y_test, cols_to_norm)\n",
    "\n",
    "node_features = G_train.ndata['h']\n",
    "edge_features = G_train.edata['h']\n",
    "\n",
    "node_features_test = G_test.ndata['h']\n",
    "edge_features_test = G_test.edata['h']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d82e6448",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(420070, 12)\n",
      "(180030, 12)\n",
      "torch.Size([840140, 1, 8])\n",
      "torch.Size([360060, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(edge_features.shape)\n",
    "print(edge_features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8d8e069",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ndim_in = G_train.ndata['h'].shape[2]\n",
    "ndim_out = 128 \n",
    "edim = G_train.ndata['h'].shape[2]\n",
    "activation = F.relu\n",
    "dropout = 0.2\n",
    "n_classes = data[cname_label].nunique()\n",
    "\n",
    "sage = SAGE(ndim_in, ndim_out, edim, activation, dropout).to(device)\n",
    "mlp = MLPPredictor(ndim_out, 2).to(device)\n",
    "model = Model(ndim_in, ndim_out, edim, n_classes, activation, dropout).to(device)\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    'balanced', \n",
    "    classes=data[cname_label].unique(),\n",
    "    y=data[cname_label].values.tolist(),\n",
    ")\n",
    "class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2dc4c9c8",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0010 - Training acc: 0.8192979693412781\n"
     ]
    }
   ],
   "source": [
    "edge_label = G_train.edata[cname_label]\n",
    "train_mask = G_train.edata['train_mask']\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    pred = model(G_train, node_features, edge_features).to(device)\n",
    "    loss = criterion(pred[train_mask], edge_label[train_mask])\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'{epoch:04d} - Training acc:', compute_accuracy(pred[train_mask], edge_label[train_mask]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c93a95c7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_pred_prop = model(G_train, node_features, edge_features).to(device)\n",
    "train_pred = train_pred_prop.argmax(1)\n",
    "train_pred = torch.Tensor.cpu(train_pred).detach().numpy()\n",
    "train_actual = G_train.edata.pop(cname_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ec33433",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'actual' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [25], line 9\u001b[0m\n\u001b[1;32m      4\u001b[0m test_actual \u001b[38;5;241m=\u001b[39m G_test\u001b[38;5;241m.\u001b[39medata\u001b[38;5;241m.\u001b[39mpop(cname_label)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# actual = [\"Normal\" if i == 0 else \"Attack\" for i in actual]\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# test_pred = [\"Normal\" if i == 0 else \"Attack\" for i in test_pred]\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m actual_1 \u001b[38;5;241m=\u001b[39m [i\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mactual\u001b[49m]\n\u001b[1;32m     10\u001b[0m test_pred_1 \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m test_pred]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'actual' is not defined"
     ]
    }
   ],
   "source": [
    "test_pred_prop = model(G_test, node_features_test, edge_features_test).to(device)\n",
    "test_pred = test_pred_prop.argmax(1)\n",
    "test_pred = torch.Tensor.cpu(test_pred).detach().numpy()\n",
    "test_actual = G_test.edata.pop(cname_label)\n",
    "\n",
    "# actual = [\"Normal\" if i == 0 else \"Attack\" for i in actual]\n",
    "# test_pred = [\"Normal\" if i == 0 else \"Attack\" for i in test_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c203bf6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "norm_test_pred_prop = torch.softmax(test_pred_prop, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c7b110c",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.6942e+01, -4.6637e+01],\n",
       "        [ 3.9542e+01, -4.8324e+01],\n",
       "        [ 1.9192e+01, -1.9510e+01],\n",
       "        ...,\n",
       "        [ 1.3778e+00, -1.5827e+00],\n",
       "        [ 2.9585e+00, -3.1632e+00],\n",
       "        [-1.5411e-01, -1.5061e-02]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68a5bd8f",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 5.0369e-37],\n",
       "        [1.0000e+00, 6.9235e-39],\n",
       "        [1.0000e+00, 1.5552e-17],\n",
       "        ...,\n",
       "        [9.5076e-01, 4.9239e-02],\n",
       "        [9.9781e-01, 2.1900e-03],\n",
       "        [4.6529e-01, 5.3471e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_test_pred_prop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "222px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
